{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LP_DeepFake_milestone 3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdbsPh6deb8h",
        "colab_type": "text"
      },
      "source": [
        "# Milestone 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsiRtOJseexL",
        "colab_type": "text"
      },
      "source": [
        "Now you can compute a feature vector for a single image, which is the cropped face from a video frame.\n",
        "\n",
        "The goal now is, **for each video and for each frame** of the video, to **detect the face** in the video, **compute the features** for that face, and save the resulted feature on disk in **HDF5 file**. \n",
        "\n",
        "You should have one HDF5 file for each video. \n",
        "\n",
        "\\\\\n",
        "The file will contain a **matrix** with the number of **rows** equal to the number of **frames** in that video and the number of **columns** equal to the number of **features** you compute for a single face.\n",
        "\n",
        "The **HDF5 files should be saved in the same directory structure that the video database has**, but instead of videos you will have HDF5 files with features.\n",
        "\n",
        "To loop through the videos inside a directory, you can use standard python routines for recursively traversing the directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IF58SPPOX_k6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import tarfile\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "# some settings to make it smoothly runnable in Jupyter\n",
        "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esOl7h51gLQV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "cfa38639-969f-47c0-f1d8-8c03beb3e846"
      },
      "source": [
        "!wget https://liveproject-resources.s3.amazonaws.com/other/detectingdeepfakes/DeepfakeTIMIT.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-21 16:08:34--  https://liveproject-resources.s3.amazonaws.com/other/detectingdeepfakes/DeepfakeTIMIT.tar.gz\n",
            "Resolving liveproject-resources.s3.amazonaws.com (liveproject-resources.s3.amazonaws.com)... 52.216.251.76\n",
            "Connecting to liveproject-resources.s3.amazonaws.com (liveproject-resources.s3.amazonaws.com)|52.216.251.76|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 226611200 (216M) [application/x-gzip]\n",
            "Saving to: ‘DeepfakeTIMIT.tar.gz’\n",
            "\n",
            "DeepfakeTIMIT.tar.g 100%[===================>] 216.11M  63.7MB/s    in 3.4s    \n",
            "\n",
            "2020-07-21 16:08:37 (63.7 MB/s) - ‘DeepfakeTIMIT.tar.gz’ saved [226611200/226611200]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNAAbfV3Xzni",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "188fbec0-e923-4895-dd5b-b4ed45847dc9"
      },
      "source": [
        "# Extract the files\n",
        "filename = \"DeepfakeTIMIT.tar.gz\"\n",
        "tf = tarfile.open(filename)\n",
        "tf.extractall()\n",
        "\n",
        "os.listdir('DeepfakeTIMIT')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fram1-original.mov',\n",
              " 'deepfake_images_1.png',\n",
              " 'lower_quality',\n",
              " 'deepfake_images_2.png',\n",
              " 'fadg0-fram1-roi93.mov',\n",
              " 'higher_quality',\n",
              " 'fadg0-original.mov',\n",
              " '.dircksum',\n",
              " 'README.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDIODIF6Y4kp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        },
        "outputId": "f17bd8c0-bdb9-42f5-aefc-bbd8e4191cb7"
      },
      "source": [
        "# Download and unzip the VidTIMIT dataset\n",
        "!wget https://liveproject-resources.s3.amazonaws.com/other/detectingdeepfakes/VidTIMIT.zip\n",
        "\n",
        "!unzip -q VidTIMIT.zip\n",
        "os.listdir('VidTIMIT')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-21 16:08:45--  https://liveproject-resources.s3.amazonaws.com/other/detectingdeepfakes/VidTIMIT.zip\n",
            "Resolving liveproject-resources.s3.amazonaws.com (liveproject-resources.s3.amazonaws.com)... 52.216.147.124\n",
            "Connecting to liveproject-resources.s3.amazonaws.com (liveproject-resources.s3.amazonaws.com)|52.216.147.124|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1358810924 (1.3G) [application/zip]\n",
            "Saving to: ‘VidTIMIT.zip’\n",
            "\n",
            "VidTIMIT.zip        100%[===================>]   1.26G  43.8MB/s    in 32s     \n",
            "\n",
            "2020-07-21 16:09:18 (40.0 MB/s) - ‘VidTIMIT.zip’ saved [1358810924/1358810924]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fcmh0',\n",
              " 'mrgg0',\n",
              " 'mdbb0',\n",
              " 'felc0',\n",
              " 'fcmr0',\n",
              " 'mrcz0',\n",
              " 'msjs1',\n",
              " 'fgjd0',\n",
              " 'mbdg0',\n",
              " 'mbjk0',\n",
              " 'fdac1',\n",
              " 'mmdb1',\n",
              " 'mcem0',\n",
              " 'mdab0',\n",
              " 'mabw0',\n",
              " 'mwbt0',\n",
              " 'fjre0',\n",
              " 'fdms0',\n",
              " 'fjas0',\n",
              " 'fram1',\n",
              " 'fkms0',\n",
              " 'mrjo0',\n",
              " 'fadg0',\n",
              " 'mmdm2',\n",
              " 'mpdf0',\n",
              " 'fcft0',\n",
              " 'mjar0',\n",
              " 'mgwt0',\n",
              " 'mtas1',\n",
              " 'fpkt0',\n",
              " 'fedw0',\n",
              " 'mreb0',\n",
              " 'mtmr0',\n",
              " 'faks0',\n",
              " 'mjsw0',\n",
              " 'mccs0',\n",
              " 'mpgl0',\n",
              " 'mstk0',\n",
              " 'fcrh0',\n",
              " 'fdrd1',\n",
              " 'fjwb0',\n",
              " 'mdld0',\n",
              " 'fjem0']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYJDwL2rUkZx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8a524356-bc81-4639-8c52-48749da85efc"
      },
      "source": [
        "# Real videos\n",
        "real_dir = 'VidTIMIT'\n",
        "real_videos_list = glob.glob(real_dir + '/**/*.avi', recursive=True)\n",
        "print('No. of real videos: {}'.format(len(real_videos_list)))\n",
        "\n",
        "# Fake videos\n",
        "# we focus on high quality videos of DeepfakeTIMIT dataset\n",
        "fake_dir_hq = 'DeepfakeTIMIT/higher_quality'\n",
        "fake_videos_list = glob.glob(fake_dir_hq + '/**/*.avi', recursive=True)\n",
        "print('No. of HQ fake videos: {}'.format(len(fake_videos_list)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of real videos: 430\n",
            "No. of HQ fake videos: 320\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZnAQ-XidcK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sort the lists of videos\n",
        "real_videos_list = sorted(real_videos_list)\n",
        "fake_videos_list = sorted(fake_videos_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q3yiYpnRE9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# How many frames per video?\n",
        "def count_video_frames(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    return num_frames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgKXUDvkRYB7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "acc302eb-763d-433f-c0d0-e56ca2083ef2"
      },
      "source": [
        "# Num frames REAL\n",
        "f = [count_video_frames(video) for video in real_videos_list]\n",
        "print('Minimum number of frames in real videos:', min(f))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Minimum number of frames in real videos: 54\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PzpseqCSJD8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fdd46a6a-3675-4bc6-842a-800c87fa28f2"
      },
      "source": [
        "# Num frames FAKE\n",
        "f = [count_video_frames(video) for video in fake_videos_list]\n",
        "print('Minimum number of frames in fake videos:', min(f))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Minimum number of frames in fake videos: 54\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5O41aKdpvgX",
        "colab_type": "text"
      },
      "source": [
        "## Detect faces and extract features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAMBqkX-ci2D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "0e9339e9-ad12-45a6-8440-5ef5c2a82009"
      },
      "source": [
        "# Install the face detector\n",
        "!pip install mtcnn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mtcnn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/43/abee91792797c609c1bf30f1112117f7a87a713ebaa6ec5201d5555a73ef/mtcnn-0.1.0-py3-none-any.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: opencv-python>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from mtcnn) (4.1.2.30)\n",
            "Requirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from mtcnn) (2.3.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python>=4.1.0->mtcnn) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.0->mtcnn) (1.4.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.0->mtcnn) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.0->mtcnn) (1.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.0->mtcnn) (2.10.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.0->mtcnn) (1.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.0->mtcnn) (3.13)\n",
            "Installing collected packages: mtcnn\n",
            "Successfully installed mtcnn-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ9LwmTSJS00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from mtcnn import MTCNN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbFjvNWPcT3m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def detect_face(image, box_scale=0.15):\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    detection_result = detector.detect_faces(image_rgb)\n",
        "    # get a box around the face\n",
        "    if len(detection_result) > 0:  # if a face is detected\n",
        "        bounding_box = detection_result[0]['box']\n",
        "        # enlarge detection box by the given scale\n",
        "        x = int(bounding_box[0] - box_scale * bounding_box[2])\n",
        "        y = int(bounding_box[1] - box_scale * bounding_box[3])\n",
        "        w = int(bounding_box[2] + box_scale * bounding_box[2] * 2)\n",
        "        h = int(bounding_box[3] + box_scale * bounding_box[3] * 2)\n",
        "        # crop a face\n",
        "        return image[y:y+h, x:x+w, :].copy()\n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70j3bSi8JBQw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute features\n",
        "\n",
        "# Note that the number of bins we use for the histogram is a parameter of the system\n",
        "# more bins - more features\n",
        "def compute_hist(image, num_bins=64):\n",
        "    hist, bins = np.histogram(image.ravel(), num_bins, [0,256], density=True)\n",
        "    return hist\n",
        "\n",
        "import skimage.metrics\n",
        "num_hist_bins = 64\n",
        "\n",
        "def compute_blurred_image(image, kernel_size=3, sigma=0.5):\n",
        "    return cv2.GaussianBlur(image, (kernel_size, kernel_size), sigma)\n",
        "\n",
        "def mse(x, y):\n",
        "    return skimage.metrics.normalized_root_mse(x, y)\n",
        "\n",
        "def psnr(x, y):\n",
        "    return skimage.metrics.peak_signal_noise_ratio(x, y, data_range=255)\n",
        "\n",
        "def ssim(x, y):\n",
        "    return skimage.metrics.structural_similarity(x, y, multichannel=True, \n",
        "                                                 gaussian_weights=True, sigma=1.5, \n",
        "                                                 use_sample_covariance=False, data_range=255)\n",
        "\n",
        "def compute_features(image):\n",
        "    image_blurred = compute_blurred_image(image)\n",
        "    im_ssim = ssim(image, image_blurred)\n",
        "    im_mse = mse(image, image_blurred)\n",
        "    im_psnr = psnr(image, image_blurred)\n",
        "    im_hist = compute_hist(image, num_bins=num_hist_bins)\n",
        "    features = np.concatenate([[im_ssim], [im_mse], [im_psnr], im_hist])\n",
        "    return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "To2DWquDYnsB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  def detect_and_extract_features(video_path, box_scale=0.15, limit_faces=-1):\n",
        "    detector = MTCNN()\n",
        "    \n",
        "    video_features = np.zeros((54, 67)) # 54 frames, 67 features  \n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    num_frames = 54\n",
        "\n",
        "    for frame_no in range(num_frames):\n",
        "        # if the given limit is not -1, loop only until the limit\n",
        "        if limit_faces != -1 and frame_no >= limit_faces:\n",
        "            break\n",
        "        \n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_no)\n",
        "        ret, frame = cap.read()\n",
        "        \n",
        "        # detect faces\n",
        "        face = detect_face(frame, box_scale=box_scale)\n",
        "        if face is not None:\n",
        "            video_features[frame_no] = compute_features(face)\n",
        "    \n",
        "    video_features = np.reshape(video_features, (1, 54*67 ))\n",
        "\n",
        "    return video_features\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooeYLK-1g5Pc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "22212bbb-678a-4b08-87cf-b7396c0e5225"
      },
      "source": [
        "# extract features of all REAL videos\n",
        "real_features_mx = np.zeros((len(real_videos_list), 54*67))\n",
        "\n",
        "for i in tqdm(range(len(real_videos_list))):\n",
        "  real_features_mx[i] = detect_and_extract_features(real_videos_list[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 430/430 [1:13:56<00:00, 10.32s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XqIVwJvz_hJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0ee2b778-5556-4dba-d7c3-1c1da74d64cb"
      },
      "source": [
        "# extract features of all FAKE videos\n",
        "fake_features_mx = np.zeros((len(fake_videos_list), 54*67))\n",
        "\n",
        "for i in tqdm(range(len(fake_videos_list))):\n",
        "  fake_features_mx[i] = detect_and_extract_features(fake_videos_list[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 320/320 [54:10<00:00, 10.16s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uj4FgX967f2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import h5py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZex3J_L7hv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save features as HDF5 files\n",
        "# Real videos\n",
        "hdf5_path = 'real_videos_features.h5'\n",
        "with h5py.File(hdf5_path, 'w') as hf: \n",
        "    hf.create_dataset(name='real_features', data=real_features_mx)\n",
        "\n",
        "# Fake videos\n",
        "hdf5_path = 'fake_videos_features.h5'\n",
        "with h5py.File(hdf5_path, 'w') as hf: \n",
        "    hf.create_dataset(name='fake_features', data=fake_features_mx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4pDr1bxB2FN",
        "colab_type": "text"
      },
      "source": [
        "## Train a SVM classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IvauLF3B4wk",
        "colab_type": "text"
      },
      "source": [
        "Using scikit-learn train SVM classifier on the features. When reading the features from the saved HDF5 files, you need to also **construct a vector with labels that has 0 label for each Deepfake features and 1 label for each genuine feature.** \n",
        "\n",
        "You can use linear SVM and play with different parameters of this classifier and study their impact on the results.\n",
        "\n",
        "### Workflow\n",
        "\n",
        "**Split the set of videos into two sets: training and testing.** \n",
        "\n",
        "There are different ways to do it but the split of **80%** of data for training and 20% for testing is the common one. \n",
        "You can use **train_test_split()** function from sklearn.model_selection.\n",
        "\n",
        "Be careful how you split the list of videos into 80% for training and 20% for testing. \n",
        "You need to make sure that 80% of Deepfake videos are inside the training set and 80% of original videos are also inside the training set. \n",
        "\n",
        "Also, you need to **split the videos, not their features** (you have many features vectors for each video); when you evaluate later, you will need to compute one prediction score per test video, which means all features from that videos must be inside the test set. \n",
        "You must always evaluate your trained classification model on the features that you did not use for training.\n",
        "\n",
        "In a loop through all original and deepfake videos (use Python’s Glob to loop through folders), for each video compute features for all frames (loop through frames with OpenCV) in the video and save the features in HDF5 files. \n",
        "One HDF5 files should correspond to one video and should contain the feature matrix of N x M, where N is the number of frames in that video and M is the number of features you computed for on frame, so each row is a feature vector for one frame of the video.\n",
        "\n",
        "Once all features are computed, focus on the training set of videos. Loop through the stored HDF5 files (use the same Glob library) of the training set, read HDF5 files and **combine all the features vectors in one numpy array, where rows are feature vectors from all videos. **\n",
        "\n",
        "In the same time, **create a separate array of integer labels**, which would have 0 label for the feature vector corresponding to Deepfake frame and label 1 corresponding to original frame. \n",
        "\n",
        "In the end, **you should have two arrays:** \n",
        "* 1) array of features extracted from all frames of all videos and \n",
        "* 2) array of labels of the same length, where you store which feature is from fake video and which is from the original video.\n",
        "\n",
        "**Train SVM classifier of scikit-learn on the features and labels from the training set.** This trained classifier will be used in the next milestone.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wuJdBrlLT4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbKAbagfMRiO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_real = np.ones(len(real_videos_list))\n",
        "y_fake = np.zeros(len(fake_videos_list))\n",
        "\n",
        "y = np.concatenate((y_real, y_fake), axis = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JC3_MmmO_tx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0cf2032f-01f9-4c70-aeea-dbf5b22a5c24"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(750,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSmK2nfPOwJ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rvf = 'real_videos_features.h5'\n",
        "fvf = 'fake_videos_features.h5'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdX6oq1aQMU4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "82842e47-ce7e-42bc-aa72-78031d51479c"
      },
      "source": [
        "hf = h5py.File(rvf, 'r')\n",
        "hf.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KeysViewHDF5 ['real_features']>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3lD5fhQQWvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_real = hf.get('real_features')\n",
        "X_real = np.array(X_real)\n",
        "hf.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mI-rSGF9QPnb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5e1b0daa-be52-45ae-fa01-9286e25b7f1d"
      },
      "source": [
        "X_real.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(430, 3618)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6Ekj53kQN6g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "25329daa-a67c-4034-fdf1-bec6a38074f1"
      },
      "source": [
        "hf = h5py.File(fvf, 'r')\n",
        "X_fake = hf.get('fake_features')\n",
        "X_fake = np.array(X_fake)\n",
        "\n",
        "X = np.concatenate((X_real, X_fake), axis = 0)\n",
        "\n",
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(750, 3618)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4i9tBmu3PI2F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "edc8bfd6-adbb-4bf8-9d6d-17b48951779c"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)\n",
        "\n",
        "print('After splitting:\\nNo. of samples in the training set: {}\\nNo. of samples in the test set: {}'.format(X_train.shape[0], X_test.shape[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After splitting:\n",
            "No. of samples in the training set: 600\n",
            "No. of samples in the test set: 150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y6OdvskDowI",
        "colab_type": "text"
      },
      "source": [
        "Standardize numeric data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvlkHnmfDB7V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # Standardize numeric data\n",
        " # Instantiate the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Scale the columns\n",
        "X_train_scaled  = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqXQwSpLNtb7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import svm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTqNDivqNwQe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5b1d5ffd-5f32-4292-88e5-d3079d2011ae"
      },
      "source": [
        "model = svm.SVC(kernel='linear')\n",
        "model.fit(X_train_scaled, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVyBihSbR9g6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "65fee638-3c7d-4146-e21d-394037eaaf88"
      },
      "source": [
        "y_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
              "       1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
              "       0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
              "       1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
              "       1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
              "       1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
              "       1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
              "       1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4QVug4FNzUA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "ba20530e-8d0b-4ecd-ebd4-ecf4614d7455"
      },
      "source": [
        "y_pred = model.predict(X_test_scaled)\n",
        "y_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
              "       1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
              "       0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
              "       1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0.,\n",
              "       1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
              "       1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
              "       1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
              "       1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nokO8_weSgVQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "56add71a-2044-4c99-eee7-cff6178929b7"
      },
      "source": [
        "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.98\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}