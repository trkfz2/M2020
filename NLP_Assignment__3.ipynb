{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment _3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOYFxp7Fq7HARdPCwXKEYD8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trkfz2/M2020/blob/master/NLP_Assignment__3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vqDabVpPlG2",
        "colab_type": "code",
        "outputId": "52fe11bd-ad80-4959-9c6b-e5ebed9c045d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgklimX9PmfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPxub89qPrUx",
        "colab_type": "code",
        "outputId": "cdbacd99-8ac1-4a24-e09c-f53315ecb78c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.15.2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR213YdHYkrM",
        "colab_type": "text"
      },
      "source": [
        "# Preparing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xnEuLv5YJyh",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Load the dataset that was prepared in task 1\n",
        "\n",
        "The original dataset is too large and needs to be reduced. You can for instance:\n",
        " - filter out items that have too many or too little tokens\n",
        " - select items of a certain type: post, comments or titles\n",
        " - or sub sample items randomly\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SDbKN5G1lpz",
        "colab_type": "code",
        "outputId": "530ca20a-172a-48ee-81ce-4dbe0b34e98d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1XOqQHEZwN9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RK1ZD3AyV65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load data into DataFrame\n",
        "\n",
        "data = pd.read_csv('drive/My Drive/Manning Project/NLP/stackexchange_812k.tokenized.csv').sample(frac = 1, random_state = 0).reset_index(drop = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnrjIP4h5T7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert data.shape == (789649, 7), \"The dataset does not have the right dimensions\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ainzreA6--a",
        "colab_type": "code",
        "outputId": "77a76f75-87fe-4731-c48b-f849f172e1cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "list(data.columns)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['post_id',\n",
              " 'parent_id',\n",
              " 'comment_id',\n",
              " 'text',\n",
              " 'category',\n",
              " 'tokens',\n",
              " 'n_tokens']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9YrqxTV8ZIc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.options.display.max_colwidth = 500"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu7xB2fw8No1",
        "colab_type": "code",
        "outputId": "70a97715-d4da-4405-b6bd-78174d1e3d88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "# show some examples\n",
        "\n",
        "data.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_id</th>\n",
              "      <th>parent_id</th>\n",
              "      <th>comment_id</th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "      <th>tokens</th>\n",
              "      <th>n_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>298828</td>\n",
              "      <td>NaN</td>\n",
              "      <td>567970.0</td>\n",
              "      <td>That's not an option atm. This data definitely provides some evidence for length of adherence, I just don't know how to use it.</td>\n",
              "      <td>comment</td>\n",
              "      <td>that ' s not an option atm . this data definitely provides some evidence for length of adherence , i just don ' t know how to use it .</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>142749</td>\n",
              "      <td>142741.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>already provided a link to the discussion containing the theoretical aspects. Here is a quick pratical example of how one would do it in R. Please also have a look at these documents which contain the theory as well as examples Simultaneous Inference in General Parametric Models and Additional multcomp Examples . We will use the mtcars dataset and build a linear regression model containing three variables cyl Number of cylinders , disp Displacement and hp Horsepower to predict the variable m...</td>\n",
              "      <td>post</td>\n",
              "      <td>already provided a link to the discussion containing the theoretical aspects . here is a quick pratical example of how one would do it in r . please also have a look at these documents which contain the theory as well as examples simultaneous inference in general parametric models and additional multcomp examples . we will use the mtcars dataset and build a linear regression model containing three variables cyl number of cylinders , disp displacement and hp horsepower to predict the variable...</td>\n",
              "      <td>332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>49674</td>\n",
              "      <td>NaN</td>\n",
              "      <td>96088.0</td>\n",
              "      <td>As for I would say this question is off-topic and unlikely to help future visitors interested in statistical science in its theoretical and applied aspects.</td>\n",
              "      <td>comment</td>\n",
              "      <td>as for i would say this question is off - topic and unlikely to help future visitors interested in statistical science in its theoretical and applied aspects .</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>264715</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I attended a conference on ML and Data Science and I have a general question that was not answered in the conference. If we have a continuous variable, let's say age. What is the best way to handle this variable. These are my thoughts, please let me know if they nonsense, but in general I think it is a very important and useful topic that has not been discussed in the detail that I need it How should you decide on the number of bins? Would it be best to choose an arbitrary number of bins and...</td>\n",
              "      <td>post</td>\n",
              "      <td>i attended a conference on ml and data science and i have a general question that was not answered in the conference . if we have a continuous variable , let ' s say age . what is the best way to handle this variable . these are my thoughts , please let me know if they nonsense , but in general i think it is a very important and useful topic that has not been discussed in the detail that i need it how should you decide on the number of bins ? would it be best to choose an arbitrary number of...</td>\n",
              "      <td>258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>45344</td>\n",
              "      <td>NaN</td>\n",
              "      <td>88126.0</td>\n",
              "      <td>Oh well, I actually meant equivalence in terms of only observed variables -- essentially maxing or more commonly, marginalizing out latent variables and then establishing equivalence.</td>\n",
              "      <td>comment</td>\n",
              "      <td>oh well , i actually meant equivalence in terms of only observed variables -- essentially maxing or more commonly , marginalizing out latent variables and then establishing equivalence .</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   post_id  ...  n_tokens\n",
              "0   298828  ...        30\n",
              "1   142749  ...       332\n",
              "2    49674  ...        28\n",
              "3   264715  ...       258\n",
              "4    45344  ...        29\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNe54TqB2NQ0",
        "colab_type": "text"
      },
      "source": [
        "**Training set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5iqF5OnEyU8",
        "colab_type": "code",
        "outputId": "3cee70e5-7339-410c-fddf-e6e9afa8b136",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "list(data['category'].unique())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['comment', 'post', 'title']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZO28MLS6sfy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# trainign set will consist of comments and posts\n",
        "\n",
        "training = data[data.category.isin([\"post\",\"comment\"]) ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrwyfGVT7Q-8",
        "colab_type": "code",
        "outputId": "90fdaa09-beca-48d1-d912-b5fff85f3dec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "training.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(705964, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtfXrHl4ypHE",
        "colab_type": "code",
        "outputId": "f101e7fd-94d8-4c38-8df5-62ae5c4a7286",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "training.category.value_counts()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "comment    540587\n",
              "post       165377\n",
              "Name: category, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6H22fJvzyB-",
        "colab_type": "code",
        "outputId": "345fcbb6-3b32-49a2-d639-90004667ae36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(\"Number of tokens per post or comment\")\n",
        "\n",
        "print(\"Minimum: {}\".format(training['n_tokens'].min()))\n",
        "print(\"Maximum: {}\".format(training['n_tokens'].max()))\n",
        "\n",
        "print(\"Average: {:.2f}\".format(training['n_tokens'].mean()))\n",
        "print(\"Median: {}\".format(training['n_tokens'].median()))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of tokens per post or comment\n",
            "Minimum: 5\n",
            "Maximum: 4903\n",
            "Average: 67.59\n",
            "Median: 42.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2r3CfN9g0G96",
        "colab_type": "code",
        "outputId": "992d22b1-568d-4dd2-b7f3-c52194c4bd49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(\"Number of posts or comments with more than:\")\n",
        "print(\"100 tokens:{}\".format(training['n_tokens'].apply(lambda x: x > 100).sum()))\n",
        "\n",
        "print(\"500 tokens:{}\".format(training['n_tokens'].apply(lambda x: x > 500).sum()))\n",
        "\n",
        "print(\"1000 tokens:{}\".format(training['n_tokens'].apply(lambda x: x > 1000).sum()))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of posts or comments with more than:\n",
            "100 tokens:117479\n",
            "500 tokens:5517\n",
            "1000 tokens:674\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFoYm5UP1ZEl",
        "colab_type": "code",
        "outputId": "3d3eab53-bf88-499c-a990-f7685a30c8ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(\"Percentage of posts or comments with more than:\")\n",
        "print(\"100 tokens:{:.2F}%\".format(\n",
        "    training['n_tokens'].apply(lambda x: x > 100).sum()/training.shape[0]*100))\n",
        "\n",
        "print(\"500 tokens:{:.2F}%\".format(\n",
        "    training['n_tokens'].apply(lambda x: x > 500).sum()/training.shape[0]*100))\n",
        "\n",
        "print(\"1000 tokens:{:.2F}%\".format(\n",
        "    training['n_tokens'].apply(lambda x: x > 1000).sum()/training.shape[0]*100))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of posts or comments with more than:\n",
            "100 tokens:16.64%\n",
            "500 tokens:0.78%\n",
            "1000 tokens:0.10%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTDtU2vy4T4F",
        "colab_type": "code",
        "outputId": "fdb80771-ccaf-4e04-9d0b-7b2b2b5d4e5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(\"Number of posts or comments with less than:\")\n",
        "print(\"20 tokens: {}\".format(training['n_tokens'].apply(lambda x: x <20).sum()))\n",
        "\n",
        "print(\"10 tokens: {}\".format(training['n_tokens'].apply(lambda x: x < 10).sum()))\n",
        "\n",
        "print(\"6 tokens: {}\".format(training['n_tokens'].apply(lambda x: x < 6).sum()))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of posts or comments with less than:\n",
            "20 tokens: 150907\n",
            "10 tokens: 41664\n",
            "6 tokens: 6232\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1fN9_OY4Cc9",
        "colab_type": "code",
        "outputId": "038fa014-ac6d-4fe7-c06e-1c6f64d179d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(\"Percentage of posts or comments with less than:\")\n",
        "print(\"20 tokens:{:.2F}%\".format(training['n_tokens'].apply(lambda x: x < 20).sum()/training.shape[0]))\n",
        "\n",
        "print(\"10 tokens:{:.2F}%\".format(training['n_tokens'].apply(lambda x: x <10).sum()/training.shape[0]))\n",
        "\n",
        "print(\"6 tokens:{:.2F}%\".format(training['n_tokens'].apply(lambda x: x < 6).sum()/training.shape[0]))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of posts or comments with less than:\n",
            "20 tokens:0.21%\n",
            "10 tokens:0.06%\n",
            "6 tokens:0.01%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbef82GWDNSB",
        "colab_type": "text"
      },
      "source": [
        "Remove some posts/comments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7_5LLUP4lrE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# keep only posts and comments with the number of tokens between 20 and 100\n",
        "\n",
        "training_reduced = training[(training['n_tokens'] < 100) & (training['n_tokens'] >= 20)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCvDWHDm5uFa",
        "colab_type": "code",
        "outputId": "c51424d1-0008-49ed-e226-ba3e933e1738",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "training_reduced.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(435313, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCObga3T6L5-",
        "colab_type": "code",
        "outputId": "0f9fae21-2432-4e90-a6b3-a78603c6a4d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "training_reduced.shape[0]/training.shape[0]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6166220940444556"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-IhUHUz7Kxv",
        "colab_type": "code",
        "outputId": "d88289cd-7f08-4e14-f35c-5371ce4c7d41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "training.category.value_counts()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "comment    540587\n",
              "post       165377\n",
              "Name: category, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVLaDB-77HyA",
        "colab_type": "code",
        "outputId": "e59c57a7-130b-4bb4-966f-d81f52d4790d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "training_reduced.category.value_counts()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "comment    368925\n",
              "post        66388\n",
              "Name: category, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-prXYzLT2UhF",
        "colab_type": "text"
      },
      "source": [
        "**Testing set**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWmrd2XVERh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for testing will use only titles\n",
        "\n",
        "testing = data[data.category == 'title']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlKHc8QmE48W",
        "colab_type": "code",
        "outputId": "e1982f5c-d72f-4351-951d-57520a5a8ce2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "testing.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(83685, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLucZYIsdDyj",
        "colab_type": "text"
      },
      "source": [
        "-----\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okS5fDekYZhf",
        "colab_type": "text"
      },
      "source": [
        "Build the vocabulary as the set of all unique tokens to construct the list of token indexes.\n",
        "\n",
        "Filtering on token frequency is one way to reduce the overall size of the vocabulary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZyillwDbQov",
        "colab_type": "code",
        "outputId": "a14c3dff-c607-43ad-9be5-88eeecf8e4b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "!pip install --upgrade nltk"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
            "\r\u001b[K     |▎                               | 10kB 14.3MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 4.5MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 6.4MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 8.2MB/s eta 0:00:01\r\u001b[K     |█▏                              | 51kB 5.3MB/s eta 0:00:01\r\u001b[K     |█▍                              | 61kB 6.2MB/s eta 0:00:01\r\u001b[K     |█▋                              | 71kB 7.1MB/s eta 0:00:01\r\u001b[K     |█▉                              | 81kB 7.9MB/s eta 0:00:01\r\u001b[K     |██                              | 92kB 8.7MB/s eta 0:00:01\r\u001b[K     |██▎                             | 102kB 6.7MB/s eta 0:00:01\r\u001b[K     |██▌                             | 112kB 6.7MB/s eta 0:00:01\r\u001b[K     |██▊                             | 122kB 6.7MB/s eta 0:00:01\r\u001b[K     |███                             | 133kB 6.7MB/s eta 0:00:01\r\u001b[K     |███▏                            | 143kB 6.7MB/s eta 0:00:01\r\u001b[K     |███▍                            | 153kB 6.7MB/s eta 0:00:01\r\u001b[K     |███▋                            | 163kB 6.7MB/s eta 0:00:01\r\u001b[K     |███▉                            | 174kB 6.7MB/s eta 0:00:01\r\u001b[K     |████                            | 184kB 6.7MB/s eta 0:00:01\r\u001b[K     |████▎                           | 194kB 6.7MB/s eta 0:00:01\r\u001b[K     |████▌                           | 204kB 6.7MB/s eta 0:00:01\r\u001b[K     |████▊                           | 215kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████                           | 225kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 235kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 245kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 256kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 266kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████                          | 276kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 286kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 296kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 307kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 317kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 327kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 337kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 348kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 358kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 368kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 378kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 389kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 399kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 409kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 419kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 430kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 440kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████                      | 450kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 460kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 471kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 481kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 491kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 501kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 512kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 522kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 532kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████                    | 542kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 552kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 563kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 573kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 583kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 593kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 604kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 614kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 624kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 634kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 645kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 655kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 665kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 675kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 686kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 696kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 706kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 716kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████████                | 727kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 737kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 747kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 757kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 768kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 778kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 788kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 798kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 808kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 819kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 829kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 839kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 849kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 860kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 870kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 880kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 890kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 901kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 911kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 921kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 931kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 942kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 952kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 962kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 972kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 983kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 993kB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.0MB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.0MB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.0MB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.0MB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.0MB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.1MB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.1MB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.1MB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1MB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.1MB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.1MB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.1MB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.1MB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.1MB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.1MB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.2MB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.2MB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2MB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.2MB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.2MB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.2MB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.2MB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.2MB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.2MB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.2MB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.3MB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.3MB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.3MB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.3MB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.3MB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.3MB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.3MB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.3MB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.3MB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.4MB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.4MB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.4MB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.4MB 6.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.4MB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.4MB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.4MB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.4MB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.4MB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.4MB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5MB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-cp36-none-any.whl size=1449908 sha256=d6696f62b2db2e761d0c4892013c140cc7817ee2051de1a55402100feccb3aff\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJaqo6G_bblZ",
        "colab_type": "code",
        "outputId": "41a23088-7c51-49c3-b484-cfdb59a13238",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import nltk\n",
        "nltk.__version__"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.4.5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0JCTDMashzV",
        "colab_type": "code",
        "outputId": "362b7c3b-d5d7-4e82-b5c6-4acde18d9bc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "nltk.download()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> punkt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "    Downloading package punkt to /root/nltk_data...\n",
            "      Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9Z8s9G_6YGm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we only need tokens for training\n",
        "\n",
        "training_tokens  = training_reduced['tokens']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsGrOeB2d3Rt",
        "colab_type": "code",
        "outputId": "343ec0a1-3614-4911-ab4c-c6e34bce290c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(training_tokens)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "435313"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCjTtoBn-us_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import tokenize\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p_8WiuBzWwBd",
        "colab": {}
      },
      "source": [
        "# split comments and posts into a list of sentences\n",
        "\n",
        "train_text = [list(tokenize.sent_tokenize(item)) for item in training_tokens]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKvgoD6mXTgw",
        "colab_type": "code",
        "outputId": "329483db-8e32-4fa1-9f41-ee742b44714c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#show some examples \n",
        "\n",
        "train_text[0:2]"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[\"that ' s not an option atm .\",\n",
              "  \"this data definitely provides some evidence for length of adherence , i just don ' t know how to use it .\"],\n",
              " ['as for i would say this question is off - topic and unlikely to help future visitors interested in statistical science in its theoretical and applied aspects .']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGWifkdyliZv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.lm.preprocessing import flatten"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DgBr8M3Xa_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# flatten rows into a single file\n",
        "\n",
        "flat_rows = list(flatten(train_text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qww0BkugXi0p",
        "colab_type": "code",
        "outputId": "d0ac33c2-3d88-4cc0-ef67-88498fd04f6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(flat_rows)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1304131"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnZx_DWdXlCX",
        "colab_type": "code",
        "outputId": "03680106-8251-4704-cf4e-0f8431a7bdee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "# every item is a sentence\n",
        "\n",
        "flat_rows[100:110]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the comment by was directed at experienced statisticians who have been taught to use parameter to refer to hypothetical properties of a probability model and other words like characteristics or observations to refer to the things you measure and record .',\n",
              " 'so it is really dependent on mdp because there is another proof in this book dynamic programming and markov processes by howard for completely ergodic systems in the finite horizon .',\n",
              " \"and ... the answer in the mentioned link wasn ' t covering my problem !\",\n",
              " 'given a sample of data that contains only the score frequency distribution for scores below a certain threshold , is it possible to fit a complete normal distribution so to estimate what the will be the frequency for scores above the threshold .',\n",
              " 'good try .',\n",
              " \"you can fix the problem by using in doing so i believe you ' ll discover that for this result to be true , an additional assumption about the distribution is needed namely , that it has an expectation and the expectation is finite .\",\n",
              " 'do you mean journals that focus on methods for meta - analysis or journals that focus on publishing applied meta - analyses usually with focus on a particular discipline ?',\n",
              " 'are your categorical variables independent ?',\n",
              " 'as others pointed out , logistic regression assumes independent influence of the predictors .',\n",
              " \"if you don ' t think they are independent , then i suggest that you use a method that doesn ' t have this assumption .\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26Du12TDnC30",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the length of sentences\n",
        "\n",
        "sent_length = [len(item) for item in flat_rows]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0glLq_9xnK3w",
        "colab_type": "code",
        "outputId": "90b7a80d-e6ce-41ef-e5d5-21e025120865",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "max(sent_length)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1042"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dba2JuxWnNmb",
        "colab_type": "code",
        "outputId": "f582f14c-116d-4944-8098-d4fb020f4d8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "min(sent_length)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ctfOHDOnTlX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "# count the number of sentences given the length\n",
        "\n",
        "sent_length_count = Counter(len(item) for item in flat_rows)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLsNrCxQnnJd",
        "colab_type": "code",
        "outputId": "5bbabde5-7345-4963-d4dd-15f7e74cdbf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sorted(sent_length_count)\n",
        "sent_length_count"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({1: 23585,\n",
              "         2: 552,\n",
              "         3: 29819,\n",
              "         4: 4466,\n",
              "         5: 6718,\n",
              "         6: 5509,\n",
              "         7: 5041,\n",
              "         8: 10823,\n",
              "         9: 4665,\n",
              "         10: 3301,\n",
              "         11: 8163,\n",
              "         12: 5069,\n",
              "         13: 5530,\n",
              "         14: 6323,\n",
              "         15: 5888,\n",
              "         16: 5715,\n",
              "         17: 6289,\n",
              "         18: 6317,\n",
              "         19: 6758,\n",
              "         20: 5770,\n",
              "         21: 7919,\n",
              "         22: 7905,\n",
              "         23: 8056,\n",
              "         24: 8347,\n",
              "         25: 8655,\n",
              "         26: 8503,\n",
              "         27: 9054,\n",
              "         28: 9177,\n",
              "         29: 9195,\n",
              "         30: 9357,\n",
              "         31: 9187,\n",
              "         32: 9203,\n",
              "         33: 9587,\n",
              "         34: 9468,\n",
              "         35: 9668,\n",
              "         36: 9832,\n",
              "         37: 10019,\n",
              "         38: 9847,\n",
              "         39: 9953,\n",
              "         40: 9905,\n",
              "         41: 10159,\n",
              "         42: 10239,\n",
              "         43: 10317,\n",
              "         44: 10052,\n",
              "         45: 10188,\n",
              "         46: 10286,\n",
              "         47: 10498,\n",
              "         48: 10182,\n",
              "         49: 10393,\n",
              "         50: 10283,\n",
              "         51: 10326,\n",
              "         52: 10298,\n",
              "         53: 10446,\n",
              "         54: 10203,\n",
              "         55: 10283,\n",
              "         56: 10124,\n",
              "         57: 10097,\n",
              "         58: 9965,\n",
              "         59: 10119,\n",
              "         60: 9994,\n",
              "         61: 9975,\n",
              "         62: 9957,\n",
              "         63: 9912,\n",
              "         64: 9872,\n",
              "         65: 9662,\n",
              "         66: 9606,\n",
              "         67: 9673,\n",
              "         68: 9770,\n",
              "         69: 9600,\n",
              "         70: 9504,\n",
              "         71: 9395,\n",
              "         72: 9174,\n",
              "         73: 9302,\n",
              "         74: 9262,\n",
              "         75: 9095,\n",
              "         76: 9214,\n",
              "         77: 9099,\n",
              "         78: 8980,\n",
              "         79: 8710,\n",
              "         80: 8837,\n",
              "         81: 8636,\n",
              "         82: 8637,\n",
              "         83: 8523,\n",
              "         84: 8487,\n",
              "         85: 8376,\n",
              "         86: 8431,\n",
              "         87: 8213,\n",
              "         88: 8209,\n",
              "         89: 8219,\n",
              "         90: 8033,\n",
              "         91: 7962,\n",
              "         92: 7810,\n",
              "         93: 7932,\n",
              "         94: 7566,\n",
              "         95: 7901,\n",
              "         96: 7424,\n",
              "         97: 7386,\n",
              "         98: 7330,\n",
              "         99: 7535,\n",
              "         100: 7408,\n",
              "         101: 7172,\n",
              "         102: 7164,\n",
              "         103: 7055,\n",
              "         104: 6989,\n",
              "         105: 6752,\n",
              "         106: 6979,\n",
              "         107: 6718,\n",
              "         108: 6608,\n",
              "         109: 6562,\n",
              "         110: 6498,\n",
              "         111: 6429,\n",
              "         112: 6314,\n",
              "         113: 6177,\n",
              "         114: 6171,\n",
              "         115: 6048,\n",
              "         116: 6057,\n",
              "         117: 5859,\n",
              "         118: 5829,\n",
              "         119: 5667,\n",
              "         120: 5481,\n",
              "         121: 5586,\n",
              "         122: 5420,\n",
              "         123: 5445,\n",
              "         124: 5280,\n",
              "         125: 5154,\n",
              "         126: 5148,\n",
              "         127: 5000,\n",
              "         128: 4959,\n",
              "         129: 4899,\n",
              "         130: 4713,\n",
              "         131: 4811,\n",
              "         132: 4857,\n",
              "         133: 4412,\n",
              "         134: 4527,\n",
              "         135: 4236,\n",
              "         136: 4195,\n",
              "         137: 4160,\n",
              "         138: 4161,\n",
              "         139: 4032,\n",
              "         140: 4001,\n",
              "         141: 3857,\n",
              "         142: 3792,\n",
              "         143: 3697,\n",
              "         144: 3619,\n",
              "         145: 3629,\n",
              "         146: 3453,\n",
              "         147: 3522,\n",
              "         148: 3398,\n",
              "         149: 3156,\n",
              "         150: 3336,\n",
              "         151: 3156,\n",
              "         152: 3175,\n",
              "         153: 3102,\n",
              "         154: 3032,\n",
              "         155: 2921,\n",
              "         156: 2896,\n",
              "         157: 2828,\n",
              "         158: 2881,\n",
              "         159: 2731,\n",
              "         160: 2611,\n",
              "         161: 2617,\n",
              "         162: 2570,\n",
              "         163: 2399,\n",
              "         164: 2520,\n",
              "         165: 2331,\n",
              "         166: 2387,\n",
              "         167: 2355,\n",
              "         168: 2246,\n",
              "         169: 2216,\n",
              "         170: 2133,\n",
              "         171: 2181,\n",
              "         172: 2125,\n",
              "         173: 2025,\n",
              "         174: 2049,\n",
              "         175: 1960,\n",
              "         176: 1964,\n",
              "         177: 1828,\n",
              "         178: 1811,\n",
              "         179: 1846,\n",
              "         180: 1779,\n",
              "         181: 1701,\n",
              "         182: 1890,\n",
              "         183: 1683,\n",
              "         184: 1615,\n",
              "         185: 1568,\n",
              "         186: 1550,\n",
              "         187: 1520,\n",
              "         188: 1555,\n",
              "         189: 1467,\n",
              "         190: 1421,\n",
              "         191: 1386,\n",
              "         192: 1373,\n",
              "         193: 1327,\n",
              "         194: 1321,\n",
              "         195: 1260,\n",
              "         196: 1291,\n",
              "         197: 1188,\n",
              "         198: 1214,\n",
              "         199: 1162,\n",
              "         200: 1121,\n",
              "         201: 1112,\n",
              "         202: 1034,\n",
              "         203: 1063,\n",
              "         204: 1069,\n",
              "         205: 978,\n",
              "         206: 1067,\n",
              "         207: 1021,\n",
              "         208: 963,\n",
              "         209: 934,\n",
              "         210: 898,\n",
              "         211: 923,\n",
              "         212: 896,\n",
              "         213: 876,\n",
              "         214: 868,\n",
              "         215: 795,\n",
              "         216: 804,\n",
              "         217: 786,\n",
              "         218: 788,\n",
              "         219: 777,\n",
              "         220: 719,\n",
              "         221: 750,\n",
              "         222: 700,\n",
              "         223: 706,\n",
              "         224: 707,\n",
              "         225: 727,\n",
              "         226: 658,\n",
              "         227: 672,\n",
              "         228: 572,\n",
              "         229: 592,\n",
              "         230: 626,\n",
              "         231: 575,\n",
              "         232: 563,\n",
              "         233: 533,\n",
              "         234: 558,\n",
              "         235: 537,\n",
              "         236: 558,\n",
              "         237: 523,\n",
              "         238: 501,\n",
              "         239: 466,\n",
              "         240: 435,\n",
              "         241: 455,\n",
              "         242: 460,\n",
              "         243: 415,\n",
              "         244: 440,\n",
              "         245: 410,\n",
              "         246: 408,\n",
              "         247: 396,\n",
              "         248: 376,\n",
              "         249: 457,\n",
              "         250: 332,\n",
              "         251: 353,\n",
              "         252: 326,\n",
              "         253: 348,\n",
              "         254: 342,\n",
              "         255: 349,\n",
              "         256: 342,\n",
              "         257: 327,\n",
              "         258: 352,\n",
              "         259: 289,\n",
              "         260: 285,\n",
              "         261: 317,\n",
              "         262: 311,\n",
              "         263: 309,\n",
              "         264: 306,\n",
              "         265: 257,\n",
              "         266: 307,\n",
              "         267: 251,\n",
              "         268: 254,\n",
              "         269: 233,\n",
              "         270: 247,\n",
              "         271: 254,\n",
              "         272: 212,\n",
              "         273: 240,\n",
              "         274: 226,\n",
              "         275: 216,\n",
              "         276: 224,\n",
              "         277: 191,\n",
              "         278: 197,\n",
              "         279: 197,\n",
              "         280: 184,\n",
              "         281: 190,\n",
              "         282: 190,\n",
              "         283: 185,\n",
              "         284: 180,\n",
              "         285: 193,\n",
              "         286: 190,\n",
              "         287: 191,\n",
              "         288: 174,\n",
              "         289: 167,\n",
              "         290: 157,\n",
              "         291: 185,\n",
              "         292: 142,\n",
              "         293: 173,\n",
              "         294: 125,\n",
              "         295: 139,\n",
              "         296: 138,\n",
              "         297: 139,\n",
              "         298: 138,\n",
              "         299: 128,\n",
              "         300: 115,\n",
              "         301: 121,\n",
              "         302: 105,\n",
              "         303: 122,\n",
              "         304: 106,\n",
              "         305: 108,\n",
              "         306: 113,\n",
              "         307: 114,\n",
              "         308: 111,\n",
              "         309: 96,\n",
              "         310: 86,\n",
              "         311: 106,\n",
              "         312: 99,\n",
              "         313: 90,\n",
              "         314: 100,\n",
              "         315: 92,\n",
              "         316: 94,\n",
              "         317: 95,\n",
              "         318: 89,\n",
              "         319: 74,\n",
              "         320: 83,\n",
              "         321: 75,\n",
              "         322: 87,\n",
              "         323: 82,\n",
              "         324: 82,\n",
              "         325: 69,\n",
              "         326: 71,\n",
              "         327: 62,\n",
              "         328: 68,\n",
              "         329: 64,\n",
              "         330: 61,\n",
              "         331: 60,\n",
              "         332: 74,\n",
              "         333: 43,\n",
              "         334: 57,\n",
              "         335: 68,\n",
              "         336: 56,\n",
              "         337: 45,\n",
              "         338: 46,\n",
              "         339: 55,\n",
              "         340: 48,\n",
              "         341: 52,\n",
              "         342: 39,\n",
              "         343: 39,\n",
              "         344: 61,\n",
              "         345: 40,\n",
              "         346: 51,\n",
              "         347: 40,\n",
              "         348: 45,\n",
              "         349: 36,\n",
              "         350: 38,\n",
              "         351: 38,\n",
              "         352: 34,\n",
              "         353: 42,\n",
              "         354: 46,\n",
              "         355: 32,\n",
              "         356: 40,\n",
              "         357: 43,\n",
              "         358: 28,\n",
              "         359: 47,\n",
              "         360: 46,\n",
              "         361: 37,\n",
              "         362: 35,\n",
              "         363: 33,\n",
              "         364: 28,\n",
              "         365: 25,\n",
              "         366: 35,\n",
              "         367: 33,\n",
              "         368: 26,\n",
              "         369: 31,\n",
              "         370: 30,\n",
              "         371: 31,\n",
              "         372: 22,\n",
              "         373: 31,\n",
              "         374: 18,\n",
              "         375: 21,\n",
              "         376: 31,\n",
              "         377: 24,\n",
              "         378: 26,\n",
              "         379: 17,\n",
              "         380: 20,\n",
              "         381: 29,\n",
              "         382: 27,\n",
              "         383: 10,\n",
              "         384: 18,\n",
              "         385: 16,\n",
              "         386: 18,\n",
              "         387: 23,\n",
              "         388: 23,\n",
              "         389: 16,\n",
              "         390: 18,\n",
              "         391: 16,\n",
              "         392: 17,\n",
              "         393: 17,\n",
              "         394: 16,\n",
              "         395: 19,\n",
              "         396: 20,\n",
              "         397: 15,\n",
              "         398: 19,\n",
              "         399: 15,\n",
              "         400: 13,\n",
              "         401: 12,\n",
              "         402: 12,\n",
              "         403: 13,\n",
              "         404: 10,\n",
              "         405: 14,\n",
              "         406: 13,\n",
              "         407: 19,\n",
              "         408: 9,\n",
              "         409: 17,\n",
              "         410: 17,\n",
              "         411: 10,\n",
              "         412: 12,\n",
              "         413: 13,\n",
              "         414: 6,\n",
              "         415: 13,\n",
              "         416: 13,\n",
              "         417: 9,\n",
              "         418: 10,\n",
              "         419: 12,\n",
              "         420: 7,\n",
              "         421: 9,\n",
              "         422: 8,\n",
              "         423: 13,\n",
              "         424: 9,\n",
              "         425: 8,\n",
              "         426: 9,\n",
              "         427: 7,\n",
              "         428: 11,\n",
              "         429: 7,\n",
              "         430: 8,\n",
              "         431: 5,\n",
              "         432: 8,\n",
              "         433: 5,\n",
              "         434: 11,\n",
              "         435: 4,\n",
              "         436: 12,\n",
              "         437: 7,\n",
              "         438: 7,\n",
              "         439: 8,\n",
              "         440: 3,\n",
              "         441: 8,\n",
              "         442: 5,\n",
              "         443: 6,\n",
              "         444: 6,\n",
              "         445: 3,\n",
              "         446: 10,\n",
              "         447: 7,\n",
              "         448: 6,\n",
              "         449: 1,\n",
              "         450: 6,\n",
              "         451: 5,\n",
              "         452: 5,\n",
              "         453: 7,\n",
              "         454: 2,\n",
              "         455: 3,\n",
              "         456: 2,\n",
              "         457: 4,\n",
              "         458: 3,\n",
              "         459: 5,\n",
              "         460: 1,\n",
              "         461: 5,\n",
              "         462: 5,\n",
              "         463: 3,\n",
              "         464: 5,\n",
              "         465: 3,\n",
              "         466: 2,\n",
              "         467: 4,\n",
              "         468: 3,\n",
              "         469: 2,\n",
              "         470: 8,\n",
              "         471: 2,\n",
              "         472: 4,\n",
              "         473: 2,\n",
              "         474: 5,\n",
              "         476: 2,\n",
              "         477: 1,\n",
              "         478: 2,\n",
              "         479: 1,\n",
              "         480: 1,\n",
              "         481: 1,\n",
              "         482: 6,\n",
              "         483: 2,\n",
              "         484: 3,\n",
              "         486: 2,\n",
              "         488: 2,\n",
              "         489: 1,\n",
              "         490: 1,\n",
              "         491: 2,\n",
              "         492: 5,\n",
              "         494: 1,\n",
              "         496: 2,\n",
              "         497: 3,\n",
              "         498: 5,\n",
              "         500: 1,\n",
              "         501: 1,\n",
              "         502: 1,\n",
              "         503: 1,\n",
              "         504: 2,\n",
              "         505: 1,\n",
              "         506: 1,\n",
              "         508: 1,\n",
              "         509: 1,\n",
              "         510: 6,\n",
              "         511: 4,\n",
              "         512: 4,\n",
              "         513: 1,\n",
              "         514: 1,\n",
              "         515: 1,\n",
              "         517: 1,\n",
              "         518: 3,\n",
              "         519: 3,\n",
              "         520: 1,\n",
              "         521: 1,\n",
              "         522: 2,\n",
              "         524: 1,\n",
              "         528: 1,\n",
              "         529: 1,\n",
              "         532: 1,\n",
              "         535: 1,\n",
              "         536: 1,\n",
              "         538: 2,\n",
              "         540: 1,\n",
              "         545: 1,\n",
              "         546: 1,\n",
              "         550: 1,\n",
              "         551: 1,\n",
              "         553: 1,\n",
              "         556: 1,\n",
              "         562: 1,\n",
              "         565: 1,\n",
              "         572: 1,\n",
              "         576: 1,\n",
              "         583: 1,\n",
              "         584: 1,\n",
              "         585: 1,\n",
              "         606: 1,\n",
              "         610: 1,\n",
              "         696: 1,\n",
              "         1042: 1})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9H6G_f5wo63_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# how many sentences have less than 5 tokens?\n",
        "\n",
        "short_sent = [item for item in flat_rows if len(item) < 5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WZAOGnmpHqA",
        "colab_type": "code",
        "outputId": "375492be-c078-4e2d-bee6-f584eb1c4414",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(short_sent)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "58422"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhPoJd4OpKNL",
        "colab_type": "code",
        "outputId": "b5fd2ca6-3847-4184-972a-fdb615d7eeef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "# show some examples of short sentences\n",
        "\n",
        "short_sent[0:30]"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['e .',\n",
              " 'cf .',\n",
              " 'c -.',\n",
              " 'g .',\n",
              " 'e .',\n",
              " 'g .',\n",
              " 'g .',\n",
              " 'e .',\n",
              " '.',\n",
              " 'see',\n",
              " 'g .',\n",
              " '?',\n",
              " '?',\n",
              " 'i .',\n",
              " '?',\n",
              " 'e .',\n",
              " ', .',\n",
              " 'g .',\n",
              " 'g .',\n",
              " 'here',\n",
              " '.',\n",
              " '.',\n",
              " '.',\n",
              " '.',\n",
              " '.',\n",
              " '.',\n",
              " '?',\n",
              " 'e .',\n",
              " 'e .',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgXvLokdn-y2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# keep only sentences with number of tokens between 5 and 50\n",
        "\n",
        "flat_rows_reduced = [item for item in flat_rows if len(item)<=50 and len(item)>=5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw1ECNXCoKDw",
        "colab_type": "code",
        "outputId": "fbbd001e-bd1d-4766-db9b-0539474eaee2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(flat_rows), len(flat_rows_reduced)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1304131, 383313)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0I5VsoFMXSj",
        "colab_type": "text"
      },
      "source": [
        "-----\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QodprHkeJls9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_to_drive(x, file_name, folder):\n",
        "  import pickle, os\n",
        "  save_path = os.path.join('drive/My Drive/', folder, file_name + '.pickle')\n",
        "  print(save_path)\n",
        "  \n",
        "  with open(save_path, 'wb') as handle:\n",
        "    pickle.dump(x, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld6wTxy0KdXY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_from_drive(file_name, folder):\n",
        "  import pickle, os\n",
        "  load_path = os.path.join('drive/My Drive/', folder, file_name + '.pickle')\n",
        "  print(load_path)\n",
        "  \n",
        "  with open(load_path, 'rb') as handle:\n",
        "    return pickle.load(handle)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2FmRyRbQsrk",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3RKAuGRYdvq",
        "colab_type": "text"
      },
      "source": [
        "Set a fixed sequence length and build sequences of token indexes from the corpus. (see for instance keras pad_sequences)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJoH-qkAYguv",
        "colab_type": "text"
      },
      "source": [
        "Split the sequences into predictors and labels (keras.utils.to_categorical)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UL8FTcgtyyyu",
        "colab_type": "code",
        "outputId": "22c96fc6-df79-4f14-df5b-b6f2d783c9fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NhSlOOH3f2j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# limit hte dictionary to 1000 most used words\n",
        "\n",
        "num_words = 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQYi92tfy-Ry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(num_words = num_words, oov_token = '<UNK>' )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giEj_VXM3v_z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer.fit_on_texts(flat_rows_reduced)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ru6Hpx0lFCtq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_counts = tokenizer.word_counts\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRxB8gSLFGQH",
        "colab_type": "code",
        "outputId": "05571d34-6a5d-4ade-d8ee-eccc00692322",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "word_counts"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('that', 30476),\n",
              "             (\"'\", 41975),\n",
              "             ('s', 15046),\n",
              "             ('not', 19807),\n",
              "             ('an', 7163),\n",
              "             ('option', 346),\n",
              "             ('atm', 6),\n",
              "             ('i', 78517),\n",
              "             ('gave', 318),\n",
              "             ('you', 63161),\n",
              "             ('link', 1841),\n",
              "             ('on', 9369),\n",
              "             ('the', 84171),\n",
              "             ('whitening', 16),\n",
              "             ('transform', 222),\n",
              "             ('no', 5967),\n",
              "             ('cannot', 757),\n",
              "             ('g', 1674),\n",
              "             ('b', 2531),\n",
              "             ('hat', 128),\n",
              "             ('is', 60511),\n",
              "             ('read', 1756),\n",
              "             ('in', 22061),\n",
              "             ('as', 7587),\n",
              "             ('if', 10154),\n",
              "             ('it', 35958),\n",
              "             ('were', 939),\n",
              "             ('data', 9505),\n",
              "             ('yea', 40),\n",
              "             ('a', 43565),\n",
              "             ('pretty', 664),\n",
              "             ('tiny', 44),\n",
              "             ('dataset', 798),\n",
              "             ('more', 6482),\n",
              "             ('then', 3838),\n",
              "             ('do', 17070),\n",
              "             ('twang', 7),\n",
              "             ('great', 2125),\n",
              "             ('for', 39372),\n",
              "             ('small', 718),\n",
              "             ('ish', 22),\n",
              "             ('samples', 676),\n",
              "             ('thanks', 29245),\n",
              "             ('suggestion', 948),\n",
              "             ('thank', 15316),\n",
              "             ('your', 23023),\n",
              "             ('post', 2359),\n",
              "             ('lsm', 14),\n",
              "             ('poly', 42),\n",
              "             ('peter', 311),\n",
              "             ('reply', 1826),\n",
              "             ('from', 4610),\n",
              "             ('what', 24748),\n",
              "             ('recall', 160),\n",
              "             ('book', 902),\n",
              "             ('provides', 106),\n",
              "             ('r', 4810),\n",
              "             ('code', 1910),\n",
              "             ('yes', 6938),\n",
              "             ('added', 1008),\n",
              "             ('above', 1165),\n",
              "             ('original', 553),\n",
              "             ('c', 1970),\n",
              "             ('and', 18482),\n",
              "             ('etc', 1322),\n",
              "             ('how', 11082),\n",
              "             ('p', 4766),\n",
              "             ('value', 3057),\n",
              "             ('but', 8290),\n",
              "             ('model', 5573),\n",
              "             ('won', 327),\n",
              "             ('t', 14817),\n",
              "             ('generalize', 50),\n",
              "             ('well', 2461),\n",
              "             ('hyperparameters', 29),\n",
              "             ('distinction', 93),\n",
              "             ('are', 19692),\n",
              "             ('making', 298),\n",
              "             ('good', 4812),\n",
              "             ('try', 2596),\n",
              "             ('categorical', 253),\n",
              "             ('variables', 1875),\n",
              "             ('independent', 724),\n",
              "             ('see', 7920),\n",
              "             ('my', 10851),\n",
              "             ('answer', 12199),\n",
              "             ('below', 664),\n",
              "             ('think', 6249),\n",
              "             ('so', 10572),\n",
              "             ('they', 4138),\n",
              "             ('realted', 1),\n",
              "             ('to', 40714),\n",
              "             ('each', 1840),\n",
              "             ('other', 2470),\n",
              "             ('using', 3319),\n",
              "             ('ne', 15),\n",
              "             ('many', 2577),\n",
              "             ('all', 4641),\n",
              "             ('information', 1245),\n",
              "             ('about', 6247),\n",
              "             ('means', 1471),\n",
              "             ('lost', 195),\n",
              "             ('regression', 2162),\n",
              "             ('e', 5529),\n",
              "             ('there', 8474),\n",
              "             ('ways', 352),\n",
              "             ('mutual', 21),\n",
              "             ('one', 5674),\n",
              "             ('of', 26283),\n",
              "             ('them', 2087),\n",
              "             ('why', 6777),\n",
              "             ('just', 5430),\n",
              "             ('edit', 1764),\n",
              "             ('can', 14445),\n",
              "             ('vote', 214),\n",
              "             ('up', 2193),\n",
              "             ('agree', 1744),\n",
              "             ('first', 2316),\n",
              "             ('robpca', 2),\n",
              "             ('found', 906),\n",
              "             ('outliers', 312),\n",
              "             ('claim', 179),\n",
              "             ('have', 15989),\n",
              "             ('section', 477),\n",
              "             ('this', 33330),\n",
              "             ('paper', 1304),\n",
              "             ('best', 1233),\n",
              "             ('solution', 971),\n",
              "             ('know', 4734),\n",
              "             ('truly', 81),\n",
              "             ('some', 4629),\n",
              "             ('describe', 280),\n",
              "             ('re', 2592),\n",
              "             ('interpolating', 5),\n",
              "             ('looks', 1198),\n",
              "             ('like', 4271),\n",
              "             ('may', 2354),\n",
              "             ('used', 1782),\n",
              "             ('linear', 1248),\n",
              "             ('svm', 210),\n",
              "             ('with', 9237),\n",
              "             ('kernel', 216),\n",
              "             ('am', 7760),\n",
              "             ('now', 3388),\n",
              "             ('wondering', 227),\n",
              "             ('its', 1016),\n",
              "             ('process', 536),\n",
              "             ('properties', 102),\n",
              "             ('known', 382),\n",
              "             ('case', 3076),\n",
              "             ('here', 6692),\n",
              "             ('though', 1547),\n",
              "             ('useful', 1081),\n",
              "             ('clue', 70),\n",
              "             ('still', 2249),\n",
              "             ('contend', 2),\n",
              "             ('glm', 495),\n",
              "             ('results', 1093),\n",
              "             ('preferred', 61),\n",
              "             ('at', 5567),\n",
              "             ('clear', 2177),\n",
              "             ('final', 213),\n",
              "             ('question', 10912),\n",
              "             ('search', 418),\n",
              "             ('our', 661),\n",
              "             ('site', 1320),\n",
              "             ('could', 5313),\n",
              "             ('please', 4626),\n",
              "             ('expand', 349),\n",
              "             ('be', 15687),\n",
              "             ('considered', 294),\n",
              "             ('estimator', 285),\n",
              "             ('fdr', 55),\n",
              "             ('or', 8322),\n",
              "             ('use', 5758),\n",
              "             ('only', 2584),\n",
              "             ('defined', 379),\n",
              "             ('where', 1894),\n",
              "             ('parameter', 495),\n",
              "             ('don', 5575),\n",
              "             ('any', 6151),\n",
              "             ('right', 5229),\n",
              "             ('reporting', 36),\n",
              "             ('matrix', 1129),\n",
              "             ('obviously', 232),\n",
              "             ('transition', 46),\n",
              "             ('suppose', 972),\n",
              "             ('does', 6356),\n",
              "             ('sound', 263),\n",
              "             ('never', 602),\n",
              "             ('find', 1853),\n",
              "             ('intuitive', 158),\n",
              "             ('explanation', 1233),\n",
              "             ('tried', 1039),\n",
              "             ('zcompositions', 1),\n",
              "             ('package', 1168),\n",
              "             ('sounds', 752),\n",
              "             ('sequential', 41),\n",
              "             ('logit', 204),\n",
              "             ('me', 6021),\n",
              "             ('kindly', 128),\n",
              "             ('unmark', 1),\n",
              "             ('sample', 2479),\n",
              "             ('variance', 1209),\n",
              "             ('seems', 1955),\n",
              "             ('little', 957),\n",
              "             ('discuss', 98),\n",
              "             ('vcovhc', 7),\n",
              "             ('sandwich', 12),\n",
              "             ('exceptions', 17),\n",
              "             ('random', 1723),\n",
              "             ('beta', 479),\n",
              "             ('alpha', 399),\n",
              "             ('size', 1217),\n",
              "             ('n', 2259),\n",
              "             ('inside', 72),\n",
              "             ('loop', 52),\n",
              "             ('columns', 216),\n",
              "             ('fe', 22),\n",
              "             ('ni', 11),\n",
              "             ('co', 57),\n",
              "             ('advice', 710),\n",
              "             ('sizes', 171),\n",
              "             ('various', 103),\n",
              "             ('studies', 91),\n",
              "             ('m', 6279),\n",
              "             ('sure', 3523),\n",
              "             ('else', 961),\n",
              "             ('add', 1301),\n",
              "             ('require', 142),\n",
              "             ('clarification', 675),\n",
              "             ('unfortunately', 323),\n",
              "             ('reasoning', 187),\n",
              "             ('behind', 179),\n",
              "             ('computers', 14),\n",
              "             ('slower', 11),\n",
              "             ('past', 74),\n",
              "             ('welcome', 2353),\n",
              "             ('cross', 718),\n",
              "             ('validated', 275),\n",
              "             ('kind', 1034),\n",
              "             ('answers', 1114),\n",
              "             ('again', 2493),\n",
              "             ('tell', 771),\n",
              "             ('us', 771),\n",
              "             ('prob', 145),\n",
              "             ('em', 100),\n",
              "             ('memory', 96),\n",
              "             ('sparse', 97),\n",
              "             ('matrices', 98),\n",
              "             ('parameters', 564),\n",
              "             ('learn', 398),\n",
              "             ('maybe', 1936),\n",
              "             ('variant', 27),\n",
              "             ('lasso', 224),\n",
              "             ('better', 1885),\n",
              "             ('x', 6162),\n",
              "             ('design', 284),\n",
              "             ('repeated', 191),\n",
              "             ('measures', 278),\n",
              "             ('anova', 604),\n",
              "             ('objection', 11),\n",
              "             ('idea', 1534),\n",
              "             ('’', 497),\n",
              "             ('talking', 312),\n",
              "             ('ve', 1945),\n",
              "             ('causing', 31),\n",
              "             ('confusion', 570),\n",
              "             ('which', 3079),\n",
              "             ('comment', 3217),\n",
              "             ('mine', 151),\n",
              "             ('confused', 1044),\n",
              "             ('by', 5191),\n",
              "             ('way', 3037),\n",
              "             ('op', 601),\n",
              "             ('gap', 39),\n",
              "             ('statistic', 387),\n",
              "             ('example', 4259),\n",
              "             ('cluster', 286),\n",
              "             ('visually', 31),\n",
              "             ('website', 115),\n",
              "             ('didn', 951),\n",
              "             ('give', 1668),\n",
              "             ('anything', 840),\n",
              "             ('has', 2694),\n",
              "             ('changed', 278),\n",
              "             ('auto', 385),\n",
              "             ('arima', 664),\n",
              "             ('itself', 252),\n",
              "             ('cause', 134),\n",
              "             ('change', 862),\n",
              "             ('behaviour', 36),\n",
              "             ('details', 865),\n",
              "             ('stumbled', 13),\n",
              "             ('onto', 20),\n",
              "             ('secret', 7),\n",
              "             ('plan', 86),\n",
              "             ('fit', 1271),\n",
              "             ('quasibinomial', 6),\n",
              "             ('equivalent', 234),\n",
              "             ('broad', 258),\n",
              "             ('term', 722),\n",
              "             ('exists', 124),\n",
              "             ('formula', 961),\n",
              "             ('convert', 96),\n",
              "             ('exactly', 2328),\n",
              "             ('quite', 1318),\n",
              "             ('exist', 232),\n",
              "             ('worse', 111),\n",
              "             ('examples', 519),\n",
              "             ('plotting', 95),\n",
              "             ('correct', 4323),\n",
              "             ('tables', 82),\n",
              "             ('map', 105),\n",
              "             ('individual', 167),\n",
              "             ('letter', 37),\n",
              "             ('l', 494),\n",
              "             ('comprehensive', 55),\n",
              "             ('explain', 1573),\n",
              "             ('fields', 57),\n",
              "             ('indeed', 956),\n",
              "             ('make', 2739),\n",
              "             ('things', 848),\n",
              "             ('clearer', 408),\n",
              "             ('years', 334),\n",
              "             ('monthly', 104),\n",
              "             ('desirable', 25),\n",
              "             ('forecasting', 118),\n",
              "             ('we', 4262),\n",
              "             ('did', 3468),\n",
              "             ('done', 1067),\n",
              "             ('mistaken', 105),\n",
              "             ('function', 2101),\n",
              "             ('meaning', 318),\n",
              "             ('coefficients', 321),\n",
              "             ('different', 2212),\n",
              "             ('estimation', 226),\n",
              "             ('identical', 125),\n",
              "             ('laurans', 3),\n",
              "             ('gael', 7),\n",
              "             ('since', 462),\n",
              "             ('need', 3630),\n",
              "             ('comments', 1291),\n",
              "             ('helped', 273),\n",
              "             ('much', 5120),\n",
              "             ('mean', 7372),\n",
              "             ('hi', 1894),\n",
              "             ('same', 3047),\n",
              "             ('train', 336),\n",
              "             ('val', 68),\n",
              "             ('every', 355),\n",
              "             ('time', 3038),\n",
              "             ('times', 552),\n",
              "             ('fitted', 217),\n",
              "             ('underperforming', 1),\n",
              "             ('expecting', 49),\n",
              "             ('describing', 53),\n",
              "             ('algorithm', 479),\n",
              "             ('ah', 714),\n",
              "             ('notice', 163),\n",
              "             ('determinant', 18),\n",
              "             ('sign', 204),\n",
              "             ('eigenvalues', 70),\n",
              "             ('equal', 511),\n",
              "             ('however', 1179),\n",
              "             ('component', 128),\n",
              "             ('drop', 134),\n",
              "             ('such', 1381),\n",
              "             ('should', 5332),\n",
              "             ('finally', 170),\n",
              "             ('mixed', 286),\n",
              "             ('models', 1227),\n",
              "             ('sense', 2882),\n",
              "             ('let', 2471),\n",
              "             ('specific', 731),\n",
              "             ('questions', 1157),\n",
              "             ('also', 4578),\n",
              "             ('possibility', 96),\n",
              "             ('stratify', 12),\n",
              "             ('makes', 1823),\n",
              "             ('big', 401),\n",
              "             ('difference', 995),\n",
              "             ('anyways', 58),\n",
              "             ('schools', 31),\n",
              "             ('suck', 9),\n",
              "             ('ask', 844),\n",
              "             ('memorize', 2),\n",
              "             ('equation', 645),\n",
              "             ('reopen', 53),\n",
              "             ('scatter', 63),\n",
              "             ('plot', 1364),\n",
              "             ('binary', 429),\n",
              "             ('dv', 167),\n",
              "             ('internet', 35),\n",
              "             ('forum', 63),\n",
              "             ('reliable', 72),\n",
              "             ('source', 412),\n",
              "             ('iid', 132),\n",
              "             ('error', 1808),\n",
              "             ('terms', 506),\n",
              "             ('collinear', 9),\n",
              "             ('isn', 1266),\n",
              "             ('close', 577),\n",
              "             ('second', 1083),\n",
              "             ('similarly', 84),\n",
              "             ('problematic', 70),\n",
              "             ('notation', 473),\n",
              "             ('concise', 41),\n",
              "             ('ordered', 109),\n",
              "             ('ranking', 58),\n",
              "             ('q', 635),\n",
              "             ('help', 6136),\n",
              "             ('will', 5863),\n",
              "             ('appreciated', 1134),\n",
              "             ('would', 8659),\n",
              "             ('boxplots', 23),\n",
              "             ('interest', 290),\n",
              "             ('happen', 303),\n",
              "             ('greatly', 246),\n",
              "             ('ember', 1),\n",
              "             ('internals', 1),\n",
              "             ('perfect', 317),\n",
              "             ('helpful', 1734),\n",
              "             ('working', 505),\n",
              "             ('column', 238),\n",
              "             ('month', 125),\n",
              "             ('equations', 149),\n",
              "             ('wiki', 167),\n",
              "             ('page', 654),\n",
              "             ('y', 3083),\n",
              "             ('var', 577),\n",
              "             ('sse', 37),\n",
              "             ('sum', 815),\n",
              "             ('subsequent', 16),\n",
              "             ('hint', 402),\n",
              "             ('instead', 729),\n",
              "             ('really', 2834),\n",
              "             ('matters', 144),\n",
              "             ('adding', 255),\n",
              "             ('weights', 365),\n",
              "             ('over', 723),\n",
              "             ('sampling', 370),\n",
              "             ('something', 2620),\n",
              "             ('precise', 214),\n",
              "             ('needs', 271),\n",
              "             ('work', 1941),\n",
              "             ('too', 2056),\n",
              "             ('vague', 195),\n",
              "             ('current', 111),\n",
              "             ('form', 549),\n",
              "             ('wow', 326),\n",
              "             ('very', 6173),\n",
              "             ('out', 2908),\n",
              "             ('into', 1258),\n",
              "             ('point', 3198),\n",
              "             ('ll', 2470),\n",
              "             ('tone', 14),\n",
              "             ('down', 344),\n",
              "             ('bit', 1543),\n",
              "             ('sorry', 3481),\n",
              "             ('was', 4976),\n",
              "             ('unclear', 643),\n",
              "             ('specify', 208),\n",
              "             ('slope', 158),\n",
              "             ('age', 386),\n",
              "             ('id', 190),\n",
              "             ('lmer', 107),\n",
              "             ('hdl', 1),\n",
              "             ('sex', 97),\n",
              "             ('tanner', 3),\n",
              "             ('these', 2655),\n",
              "             ('points', 1156),\n",
              "             ('mentioned', 319),\n",
              "             ('study', 719),\n",
              "             ('wanted', 255),\n",
              "             ('clarify', 1430),\n",
              "             ('hear', 134),\n",
              "             ('thoughts', 440),\n",
              "             ('unrelated', 43),\n",
              "             ('groups', 472),\n",
              "             ('d', 2346),\n",
              "             ('catch', 112),\n",
              "             ('soon', 127),\n",
              "             ('outcome', 278),\n",
              "             ('sometimes', 227),\n",
              "             ('misleading', 146),\n",
              "             ('stated', 166),\n",
              "             ('advance', 1069),\n",
              "             ('say', 2624),\n",
              "             ('those', 1264),\n",
              "             ('costs', 34),\n",
              "             ('logistic', 436),\n",
              "             ('distribution', 1896),\n",
              "             ('applies', 148),\n",
              "             ('continuous', 623),\n",
              "             ('want', 3505),\n",
              "             ('former', 108),\n",
              "             ('single', 392),\n",
              "             ('notion', 40),\n",
              "             ('true', 1876),\n",
              "             ('depends', 911),\n",
              "             ('tasks', 41),\n",
              "             ('situation', 413),\n",
              "             ('representation', 43),\n",
              "             ('doesn', 1411),\n",
              "             ('matter', 566),\n",
              "             ('usual', 156),\n",
              "             ('generic', 60),\n",
              "             ('end', 494),\n",
              "             ('thirty', 1),\n",
              "             ('rows', 244),\n",
              "             ('writing', 142),\n",
              "             ('report', 238),\n",
              "             ('calculated', 241),\n",
              "             ('max', 367),\n",
              "             ('visit', 45),\n",
              "             ('web', 84),\n",
              "             ('sold', 23),\n",
              "             ('aweights', 2),\n",
              "             ('tips', 123),\n",
              "             ('actually', 1276),\n",
              "             ('testing', 396),\n",
              "             ('normality', 266),\n",
              "             ('per', 548),\n",
              "             ('se', 691),\n",
              "             ('both', 1960),\n",
              "             ('datasets', 125),\n",
              "             ('originally', 43),\n",
              "             ('counts', 284),\n",
              "             ('given', 1038),\n",
              "             ('candidates', 10),\n",
              "             ('check', 1460),\n",
              "             ('residuals', 452),\n",
              "             ('sort', 314),\n",
              "             ('k', 1136),\n",
              "             ('anyhow', 35),\n",
              "             ('combining', 26),\n",
              "             ('dont', 217),\n",
              "             ('fully', 179),\n",
              "             ('understand', 2953),\n",
              "             ('inexact', 2),\n",
              "             ('detailed', 509),\n",
              "             ('between', 980),\n",
              "             ('unsure', 101),\n",
              "             ('apply', 448),\n",
              "             ('top', 152),\n",
              "             ('three', 414),\n",
              "             ('homework', 373),\n",
              "             ('assumed', 159),\n",
              "             ('xreg', 25),\n",
              "             ('argument', 368),\n",
              "             ('show', 787),\n",
              "             ('density', 418),\n",
              "             ('had', 822),\n",
              "             ('bad', 631),\n",
              "             ('advanced', 43),\n",
              "             ('folks', 24),\n",
              "             ('been', 934),\n",
              "             ('valuable', 100),\n",
              "             ('wrong', 2423),\n",
              "             ('another', 1064),\n",
              "             ('simple', 1027),\n",
              "             ('look', 2890),\n",
              "             ('adjusted', 124),\n",
              "             ('higher', 266),\n",
              "             ('easily', 179),\n",
              "             ('available', 323),\n",
              "             ('even', 1019),\n",
              "             ('excel', 215),\n",
              "             ('response', 2041),\n",
              "             ('might', 2147),\n",
              "             ('specifically', 199),\n",
              "             ('off', 583),\n",
              "             ('topic', 640),\n",
              "             ('likelihood', 516),\n",
              "             ('observation', 251),\n",
              "             ('edward', 10),\n",
              "             ('carney', 1),\n",
              "             ('pull', 24),\n",
              "             ('refer', 312),\n",
              "             ('confirm', 159),\n",
              "             ('lot', 2655),\n",
              "             ('guys', 188),\n",
              "             ('appreciate', 1011),\n",
              "             ('ranges', 63),\n",
              "             ('distances', 81),\n",
              "             ('set', 1813),\n",
              "             ('miles', 21),\n",
              "             ('deviations', 89),\n",
              "             ('relationship', 184),\n",
              "             ('least', 632),\n",
              "             ('squares', 165),\n",
              "             ('content', 57),\n",
              "             ('elements', 122),\n",
              "             ('their', 557),\n",
              "             ('order', 438),\n",
              "             ('guess', 1012),\n",
              "             ('must', 599),\n",
              "             ('statistician', 130),\n",
              "             ('thing', 858),\n",
              "             ('previous', 224),\n",
              "             ('remark', 82),\n",
              "             ('meant', 754),\n",
              "             ('reluctance', 1),\n",
              "             ('slide', 38),\n",
              "             ('describes', 38),\n",
              "             ('because', 813),\n",
              "             ('affects', 30),\n",
              "             ('latter', 253),\n",
              "             ('register', 26),\n",
              "             ('account', 176),\n",
              "             ('easy', 474),\n",
              "             ('roc', 171),\n",
              "             ('curve', 337),\n",
              "             ('parameterization', 19),\n",
              "             ('put', 488),\n",
              "             ('going', 663),\n",
              "             ('detail', 379),\n",
              "             ('initally', 1),\n",
              "             ('took', 170),\n",
              "             ('satisfactory', 20),\n",
              "             ('str', 34),\n",
              "             ('frame', 321),\n",
              "             ('obs', 105),\n",
              "             ('hopefully', 195),\n",
              "             ('helps', 883),\n",
              "             ('thankyou', 51),\n",
              "             ('zbicyclist', 3),\n",
              "             ('reason', 641),\n",
              "             ('developing', 12),\n",
              "             ('prediction', 370),\n",
              "             ('able', 373),\n",
              "             ('formalise', 2),\n",
              "             ('proof', 464),\n",
              "             ('property', 118),\n",
              "             ('theorem', 318),\n",
              "             ('name', 399),\n",
              "             ('hold', 215),\n",
              "             ('general', 848),\n",
              "             ('mere', 13),\n",
              "             ('coincidence', 31),\n",
              "             ('based', 543),\n",
              "             ('values', 2410),\n",
              "             ('batchnorm', 3),\n",
              "             ('kernels', 38),\n",
              "             ('someone', 655),\n",
              "             ('strong', 174),\n",
              "             ('understatement', 4),\n",
              "             ('expected', 381),\n",
              "             ('score', 540),\n",
              "             ('inference', 196),\n",
              "             ('probably', 889),\n",
              "             ('wording', 94),\n",
              "             ('def', 12),\n",
              "             ('np', 258),\n",
              "             ('zeros', 141),\n",
              "             ('shape', 156),\n",
              "             ('exp', 317),\n",
              "             ('linalg', 35),\n",
              "             ('som', 10),\n",
              "             ('software', 405),\n",
              "             ('logarithms', 29),\n",
              "             ('problem', 3588),\n",
              "             ('suddenly', 10),\n",
              "             ('indicators', 31),\n",
              "             ('raises', 24),\n",
              "             ('till', 18),\n",
              "             ('frequent', 28),\n",
              "             ('unusual', 68),\n",
              "             ('expensive', 35),\n",
              "             ('thoughtful', 44),\n",
              "             ('few', 501),\n",
              "             ('requirements', 23),\n",
              "             ('ultimate', 48),\n",
              "             ('goal', 366),\n",
              "             ('metric', 168),\n",
              "             ('wonderful', 77),\n",
              "             ('interpret', 471),\n",
              "             ('approach', 1361),\n",
              "             ('deterministic', 62),\n",
              "             ('variable', 1928),\n",
              "             ('z', 920),\n",
              "             ('total', 487),\n",
              "             ('waiting', 44),\n",
              "             ('get', 3595),\n",
              "             ('pdf', 502),\n",
              "             ('ok', 1924),\n",
              "             ('got', 1441),\n",
              "             ('log', 1003),\n",
              "             ('follows', 223),\n",
              "             ('uniform', 280),\n",
              "             ('wonder', 216),\n",
              "             ('paid', 13),\n",
              "             ('new', 1227),\n",
              "             ('neat', 80),\n",
              "             ('seen', 368),\n",
              "             ('hmm', 353),\n",
              "             ('requires', 117),\n",
              "             ('calculation', 228),\n",
              "             ('gradients', 16),\n",
              "             ('therefore', 321),\n",
              "             ('iteration', 48),\n",
              "             ('robust', 193),\n",
              "             ('plots', 415),\n",
              "             ('high', 403),\n",
              "             ('risk', 165),\n",
              "             ('less', 615),\n",
              "             ('than', 1937),\n",
              "             ('cm', 32),\n",
              "             ('enjoyed', 9),\n",
              "             ('reading', 473),\n",
              "             ('figured', 139),\n",
              "             ('weekend', 21),\n",
              "             ('varies', 48),\n",
              "             ('diagonal', 75),\n",
              "             ('part', 1031),\n",
              "             ('loss', 360),\n",
              "             ('learning', 593),\n",
              "             ('constrain', 9),\n",
              "             ('zero', 811),\n",
              "             ('annie', 1),\n",
              "             ('hall', 51),\n",
              "             ('garnered', 1),\n",
              "             ('closer', 104),\n",
              "             ('nn', 81),\n",
              "             ('issue', 895),\n",
              "             ('entries', 46),\n",
              "             ('nonzero', 21),\n",
              "             ('none', 167),\n",
              "             ('trying', 1375),\n",
              "             ('achieve', 192),\n",
              "             ('implicitly', 23),\n",
              "             ('btw', 385),\n",
              "             ('vary', 127),\n",
              "             ('subject', 340),\n",
              "             ('ols', 307),\n",
              "             ('heck', 18),\n",
              "             ('et', 496),\n",
              "             ('al', 470),\n",
              "             ('genes', 33),\n",
              "             ('brain', 51),\n",
              "             ('behavior', 77),\n",
              "             ('vol', 93),\n",
              "             ('autoencoders', 11),\n",
              "             ('draw', 220),\n",
              "             ('normal', 1152),\n",
              "             ('copy', 104),\n",
              "             ('folder', 5),\n",
              "             ('take', 1476),\n",
              "             ('perhaps', 1227),\n",
              "             ('outlier', 124),\n",
              "             ('forgot', 151),\n",
              "             ('sys', 10),\n",
              "             ('sleep', 22),\n",
              "             ('authoritative', 10),\n",
              "             ('rest', 159),\n",
              "             ('extremes', 16),\n",
              "             ('charts', 37),\n",
              "             ('being', 729),\n",
              "             ('chartjunk', 2),\n",
              "             ('histogram', 175),\n",
              "             ('images', 132),\n",
              "             ('minimum', 173),\n",
              "             ('swap', 11),\n",
              "             ('develops', 1),\n",
              "             ('analysis', 1289),\n",
              "             ('math', 497),\n",
              "             ('och', 1),\n",
              "             ('estimating', 132),\n",
              "             ('hope', 1504),\n",
              "             ('red', 161),\n",
              "             ('blue', 101),\n",
              "             ('advantages', 34),\n",
              "             ('curves', 122),\n",
              "             ('uses', 274),\n",
              "             ('output', 712),\n",
              "             ('sean', 11),\n",
              "             ('eddy', 1),\n",
              "             ('scatterplot', 48),\n",
              "             ('clustering', 398),\n",
              "             ('cv', 920),\n",
              "             ('before', 647),\n",
              "             ('solving', 86),\n",
              "             ('aurelie', 1),\n",
              "             ('na', 294),\n",
              "             ('problems', 307),\n",
              "             ('when', 1756),\n",
              "             ('shock', 10),\n",
              "             ('absorber', 1),\n",
              "             ('step', 535),\n",
              "             ('instance', 478),\n",
              "             ('lt', 850),\n",
              "             ('features', 442),\n",
              "             ('lm', 437),\n",
              "             ('glmnet', 184),\n",
              "             ('factor', 650),\n",
              "             ('cvm', 7),\n",
              "             ('build', 103),\n",
              "             ('elaborate', 647),\n",
              "             ('initial', 176),\n",
              "             ('reference', 1209),\n",
              "             ('said', 761),\n",
              "             ('believed', 5),\n",
              "             ('excellent', 473),\n",
              "             ('alternative', 346),\n",
              "             ('method', 1306),\n",
              "             ('somehow', 170),\n",
              "             ('related', 672),\n",
              "             ('class', 591),\n",
              "             ('ties', 75),\n",
              "             ('simulate', 140),\n",
              "             ('kpss', 24),\n",
              "             ('suggest', 717),\n",
              "             ('simply', 452),\n",
              "             ('remove', 259),\n",
              "             ('idd', 1),\n",
              "             ('title', 405),\n",
              "             ('th', 252),\n",
              "             ('considering', 115),\n",
              "             ('exponential', 173),\n",
              "             ('weibull', 52),\n",
              "             ('censoring', 86),\n",
              "             ('stack', 90),\n",
              "             ('regarding', 243),\n",
              "             ('predictors', 269),\n",
              "             ('unfamiliar', 21),\n",
              "             ('somewhat', 173),\n",
              "             ('odd', 130),\n",
              "             ('written', 233),\n",
              "             ('hello', 271),\n",
              "             ('assignment', 85),\n",
              "             ('possible', 1332),\n",
              "             ('maldonado', 4),\n",
              "             ('greenland', 7),\n",
              "             ('level', 667),\n",
              "             ('clarifying', 239),\n",
              "             ('assymp', 1),\n",
              "             ('sig', 113),\n",
              "             ('truncated', 78),\n",
              "             ('delete', 219),\n",
              "             ('type', 746),\n",
              "             ('manage', 25),\n",
              "             ('units', 196),\n",
              "             ('chance', 261),\n",
              "             ('tenth', 6),\n",
              "             ('flip', 48),\n",
              "             ('explicit', 124),\n",
              "             ('shifted', 23),\n",
              "             ('gamma', 272),\n",
              "             ('likely', 289),\n",
              "             ('fail', 128),\n",
              "             ('verify', 83),\n",
              "             ('length', 322),\n",
              "             ('vector', 434),\n",
              "             ('deducted', 2),\n",
              "             ('scalar', 74),\n",
              "             ('normally', 181),\n",
              "             ('distributed', 296),\n",
              "             ('basu', 7),\n",
              "             ('statistics', 813),\n",
              "             ('discipline', 9),\n",
              "             ('taking', 308),\n",
              "             ('instructions', 18),\n",
              "             ('multivariate', 189),\n",
              "             ('series', 962),\n",
              "             ('after', 775),\n",
              "             ('google', 249),\n",
              "             ('throws', 19),\n",
              "             ('bias', 427),\n",
              "             ('neurons', 38),\n",
              "             ('choose', 376),\n",
              "             ('assumption', 418),\n",
              "             ('intralayer', 1),\n",
              "             ('connections', 18),\n",
              "             ('input', 731),\n",
              "             ('hidden', 95),\n",
              "             ('layer', 120),\n",
              "             ('edited', 1135),\n",
              "             ('links', 445),\n",
              "             ('isax', 1),\n",
              "             ('newer', 13),\n",
              "             ('version', 376),\n",
              "             ('updated', 569),\n",
              "             ('fixed', 625),\n",
              "             ('missing', 1276),\n",
              "             ('theil', 7),\n",
              "             ('simulation', 309),\n",
              "             ('avoid', 179),\n",
              "             ('r²', 17),\n",
              "             ('test', 5036),\n",
              "             ('cdf', 281),\n",
              "             ('setup', 75),\n",
              "             ('ball', 43),\n",
              "             ('course', 1012),\n",
              "             ('asking', 966),\n",
              "             ('either', 614),\n",
              "             ('depending', 46),\n",
              "             ('circumstances', 37),\n",
              "             ('disclaimer', 26),\n",
              "             ('creator', 1),\n",
              "             ('estimate', 724),\n",
              "             ('worded', 43),\n",
              "             ('poorly', 50),\n",
              "             ('cogsci', 6),\n",
              "             ('improved', 53),\n",
              "             ('vs', 913),\n",
              "             ('worsened', 1),\n",
              "             ('balanced', 73),\n",
              "             ('message', 112),\n",
              "             ('stan', 58),\n",
              "             ('abbreviation', 15),\n",
              "             ('stanislas', 2),\n",
              "             ('outlines', 3),\n",
              "             ('construct', 92),\n",
              "             ('legally', 1),\n",
              "             ('diagram', 66),\n",
              "             ('significance', 262),\n",
              "             ('threshold', 249),\n",
              "             ('incorrect', 385),\n",
              "             ('significant', 458),\n",
              "             ('synonym', 21),\n",
              "             ('expansion', 28),\n",
              "             ('near', 83),\n",
              "             ('duplicates', 48),\n",
              "             ('local', 76),\n",
              "             ('university', 71),\n",
              "             ('library', 229),\n",
              "             ('edition', 103),\n",
              "             ('copyright', 4),\n",
              "             ('date', 110),\n",
              "             ('robert', 73),\n",
              "             ('george', 24),\n",
              "             ('casella', 31),\n",
              "             ('published', 71),\n",
              "             ('springer', 61),\n",
              "             ('familiar', 307),\n",
              "             ('cem', 2),\n",
              "             ('certainly', 409),\n",
              "             ('miracles', 3),\n",
              "             ('integration', 72),\n",
              "             ('understanding', 415),\n",
              "             ('agreed', 227),\n",
              "             ('mcmc', 132),\n",
              "             ('honest', 58),\n",
              "             ('shimao', 4),\n",
              "             ('anony', 22),\n",
              "             ('mousse', 17),\n",
              "             ('suggests', 84),\n",
              "             ('technique', 130),\n",
              "             ('whuber', 421),\n",
              "             ('baffled', 10),\n",
              "             ('quote', 121),\n",
              "             ('quantile', 196),\n",
              "             ('topological', 3),\n",
              "             ('quick', 412),\n",
              "             ('simpler', 181),\n",
              "             ('resources', 102),\n",
              "             ('accept', 279),\n",
              "             ('once', 371),\n",
              "             ('implement', 141),\n",
              "             ('thx', 249),\n",
              "             ('iv', 131),\n",
              "             ('fantastic', 80),\n",
              "             ('major', 61),\n",
              "             ('saturated', 11),\n",
              "             ('essentially', 147),\n",
              "             ('perfectly', 175),\n",
              "             ('observations', 660),\n",
              "             ('mizera', 1),\n",
              "             ('müller', 2),\n",
              "             ('allow', 81),\n",
              "             ('express', 58),\n",
              "             ('measure', 590),\n",
              "             ('emergence', 2),\n",
              "             ('contentious', 8),\n",
              "             ('day', 355),\n",
              "             ('looking', 1261),\n",
              "             ('index', 183),\n",
              "             ('w', 712),\n",
              "             ('levels', 269),\n",
              "             ('disagree', 311),\n",
              "             ('perceptive', 6),\n",
              "             ('track', 134),\n",
              "             ('dear', 193),\n",
              "             ('mlofton', 7),\n",
              "             ('wait', 142),\n",
              "             ('compare', 568),\n",
              "             ('browsed', 4),\n",
              "             ('research', 332),\n",
              "             ('interesting', 1632),\n",
              "             ('contr', 74),\n",
              "             ('real', 484),\n",
              "             ('intercept', 416),\n",
              "             ('longer', 113),\n",
              "             ('copula', 58),\n",
              "             ('mforecast', 1),\n",
              "             ('works', 841),\n",
              "             ('github', 43),\n",
              "             ('cran', 50),\n",
              "             ('next', 300),\n",
              "             ...])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uGwfzvFPKRdB",
        "colab": {}
      },
      "source": [
        "word_index = tokenizer.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6HiF5yeT1Z-",
        "colab_type": "code",
        "outputId": "c5a4534c-fe7f-461d-8636-8b695e02c218",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# show some mappings\n",
        "\n",
        "i = 0\n",
        "for  k, v in word_index.items():\n",
        "  print(k, \":\", v)\n",
        "  i +=1\n",
        "  if i>10:\n",
        "    break\n"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<UNK> : 1\n",
            "the : 2\n",
            "i : 3\n",
            "you : 4\n",
            "is : 5\n",
            "a : 6\n",
            "' : 7\n",
            "to : 8\n",
            "for : 9\n",
            "it : 10\n",
            "this : 11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0WBveJnkAac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get a token by its index\n",
        "\n",
        "index2word = {idx: word for (word, idx) in word_index.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8PDIrbNkIiK",
        "colab_type": "code",
        "outputId": "a5db02a2-9f15-427a-c80f-e1f5e6478372",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "index2word[1000]"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'repeated'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48VGkKjQ9FK0",
        "colab_type": "code",
        "outputId": "86e63bca-2e5a-4ccc-f82f-9845ced87e2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Unique tokens: {}'.format(len(word_index)))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens: 30513\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SV9vMKNfPPyk",
        "colab_type": "code",
        "outputId": "50ea18b7-cd5e-4552-e695-9e23a4fa9683",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab_size = num_words + 1\n",
        "vocab_size"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1001"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaMdWwweDth9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create ngram sequences for training\n",
        "\n",
        "ngram_seq = []\n",
        "for i, sentence in enumerate(flat_rows_reduced):\n",
        "  token_list = tokenizer.texts_to_sequences([sentence])[0]\n",
        "  for j in range(1, len(token_list)):\n",
        "    n_gram_sequence = token_list[:j+1]\n",
        "    ngram_seq.append(n_gram_sequence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpLb8_jAEgMO",
        "colab_type": "code",
        "outputId": "a31bbbd8-c743-40c1-e71d-f45332527382",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(ngram_seq)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1762942"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkvNo2q-EjcK",
        "colab_type": "code",
        "outputId": "685ceca5-5172-4c03-d0c3-882cd6c052c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "ngram_seq[0:20]"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[12, 7],\n",
              " [12, 7, 25],\n",
              " [12, 7, 25, 18],\n",
              " [12, 7, 25, 18, 45],\n",
              " [12, 7, 25, 18, 45, 642],\n",
              " [12, 7, 25, 18, 45, 642, 1],\n",
              " [3, 689],\n",
              " [3, 689, 4],\n",
              " [3, 689, 4, 12],\n",
              " [3, 689, 4, 12, 168],\n",
              " [3, 689, 4, 12, 168, 35],\n",
              " [3, 689, 4, 12, 168, 35, 2],\n",
              " [3, 689, 4, 12, 168, 35, 2, 1],\n",
              " [3, 689, 4, 12, 168, 35, 2, 1, 900],\n",
              " [59, 4],\n",
              " [59, 4, 320],\n",
              " [124, 1],\n",
              " [124, 1, 5],\n",
              " [124, 1, 5, 18],\n",
              " [124, 1, 5, 18, 176]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGg_78Kbkwfl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# find the sentence with most words\n",
        "\n",
        "max_seq_len = max([len(seq) for seq in ngram_seq])  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfG-C1xGmsP3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "min_seq_len = min([len(seq) for seq in ngram_seq])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EG7X9QTi-L5",
        "colab_type": "code",
        "outputId": "3ed0141c-13c4-4bdd-9cfd-0bff85e747bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# print some output to verify the above\n",
        "\n",
        "print('Original string: ', flat_rows_reduced[0])\n",
        "print('Sequence of Word Ids: ', ngram_seq[5])\n",
        "print('Word Ids back to Words: ', ' '.join([index2word[idx] for idx in ngram_seq[5]]))\n",
        "print('Max Sequence Length: ', max_seq_len)\n",
        "print('Min Sequence Length: ', min_seq_len)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original string:  that ' s not an option atm .\n",
            "Sequence of Word Ids:  [12, 7, 25, 18, 45, 642, 1]\n",
            "Word Ids back to Words:  that ' s not an option <UNK>\n",
            "Max Sequence Length:  22\n",
            "Min Sequence Length:  2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UC1hO3uzlJjW",
        "colab_type": "text"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQGruiRgytq6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_len = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOZIVg2--tPt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ADZBBcHT56E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pad sequences to be of equal length\n",
        "\n",
        "ngram_seq_pad = pad_sequences(ngram_seq, maxlen = (seq_len + 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCLCfAnYUfWA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg1cwWoiUkCl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZaW5pWAUdC5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# shuffle \n",
        "\n",
        "np.random.shuffle(ngram_seq_pad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJH_LhTElunR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "index2word[0] = '<0>'\n",
        "word_index['<0>'] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyc_co7IIZSL",
        "colab_type": "code",
        "outputId": "9deabeda-1bfe-4bf6-880b-35c5de120513",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ngram_seq_pad.shape"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1762942, 21)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ6zC-7PEPze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split to X, y\n",
        "\n",
        "X, y = ngram_seq_pad[:, :-1], ngram_seq_pad[:,  -1] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnwqbQuVIjdj",
        "colab_type": "code",
        "outputId": "fe8a9773-2e54-4991-9dde-90c394b2d196",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X.shape, y.shape"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1762942, 20), (1762942,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5shgzQNfQx7r",
        "colab_type": "text"
      },
      "source": [
        "----\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkJOwB0SE9tZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8HWaHe8EOKq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert labels to categorical\n",
        "\n",
        "y = to_categorical(y, num_classes = vocab_size) # vocab_size = num_words + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9YZ3K4LF7kJ",
        "colab_type": "code",
        "outputId": "50f993ab-e707-4ddf-9e70-add6bf30184f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X.shape, y.shape"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1762942, 20), (1762942, 1001))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoUon0UgVe3k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training set\n",
        "\n",
        "x_train = X[:1700000,:]\n",
        "y_train = y[:1700000,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYsmjFIdVkI8",
        "colab_type": "code",
        "outputId": "4240216c-4ba3-4c0a-bf8c-694b87244d38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train.shape, y_train.shape"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1700000, 20), (1700000, 1001))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s30L57aVVnGD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# validation set\n",
        "\n",
        "x_val = X[1700000:,:]\n",
        "y_val = y[1700000:,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8vHUTWQVsZZ",
        "colab_type": "code",
        "outputId": "64f0e357-ed98-45cb-fa26-ec928b797291",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_val.shape, y_val.shape"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((62942, 20), (62942, 1001))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XtbPaos5jQ_",
        "colab_type": "text"
      },
      "source": [
        "-----\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPFGnRocYnzJ",
        "colab_type": "text"
      },
      "source": [
        "# The model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEe7BmH9YPEr",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "The data is now ready to be used to fit a neural network.\n",
        "\n",
        "Define a simple Sequential model with an embedding layer, LSTM(s) and a Dense layer with softmax activation. Feel free to experiment with dropouts, different optimizers. You can use any type of neural net you want: keras, tensorflow, pytorch, …\n",
        "\n",
        "Specify the number of epochs, the batch size and other fitting parameters\n",
        "Fit the network\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8tldda7Ndb2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wWEKHflhNpj",
        "colab_type": "code",
        "outputId": "d81269da-a284-406f-f6eb-50866141b705",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab_size"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1001"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvlivaqnhP52",
        "colab_type": "code",
        "outputId": "d381ca1d-34ea-498e-ea48-7ba1a6a4f412",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "seq_len"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPvmFrmFOyw5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(vocab_size, 64, input_length = seq_len))\n",
        "model.add(LSTM(32, return_sequences = False)) \n",
        "model.add(Dense(vocab_size, activation = 'softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22PPfY0ARHQY",
        "colab_type": "code",
        "outputId": "f4319175-0b7f-4734-a4fc-13c5c23e9a9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 20, 64)            64064     \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 32)                12416     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1001)              33033     \n",
            "=================================================================\n",
            "Total params: 109,513\n",
            "Trainable params: 109,513\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d549ILhwRNok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss = 'categorical_crossentropy', optimizer  = 'adam', metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8t_D2707Rafp",
        "colab_type": "code",
        "outputId": "eb503b02-4333-4f70-a12a-bb1825bd2b71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "history = model.fit(X, y, batch_size = 32, epochs = 3, validation_data =(x_val, y_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 1762942 samples, validate on 62942 samples\n",
            "Epoch 1/3\n",
            "1762942/1762942 [==============================] - 2341s 1ms/step - loss: 3.9664 - acc: 0.2524 - val_loss: 3.7166 - val_acc: 0.2733\n",
            "Epoch 2/3\n",
            "1762942/1762942 [==============================] - 2346s 1ms/step - loss: 3.6480 - acc: 0.2817 - val_loss: 3.6202 - val_acc: 0.2818\n",
            "Epoch 3/3\n",
            "1762942/1762942 [==============================] - 2359s 1ms/step - loss: 3.5851 - acc: 0.2882 - val_loss: 3.5799 - val_acc: 0.2875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRUVSHUIReN3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ipNxxGGxZjw",
        "colab_type": "code",
        "outputId": "f31f3e88-698d-40ab-9526-83525c534f8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label = \"Training acc\")\n",
        "plt.plot(epochs, val_acc, 'b', label = \"Validation acc\")\n",
        "plt.title(\"Train and Val ACC\")\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label = \"Training loss\")\n",
        "plt.plot(epochs, val_loss, 'b', label = \"Validation loss\")\n",
        "plt.title(\"Train and Val LOSS\")\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEICAYAAABWJCMKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiIx9/8Bj+mk0QDSZUgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcwYBodoYswt",
        "colab_type": "text"
      },
      "source": [
        "# Assessing the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEHUjIscYRCr",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Write a function that generates text\n",
        "\n",
        "generate some text and take note of\n",
        " - token repetitions\n",
        " - missing punctuations\n",
        " - other anomalies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnlQIbWrAeA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate words\n",
        "\n",
        "def generate_words(model, tokenizer, seq_len, seed_text, n_words, vocab_size):\n",
        "  \n",
        "  input_text = list(seed_text.split(\" \"))\n",
        "  \n",
        "  for i in range(n_words):\n",
        "    seq = tokenizer.texts_to_sequences([input_text])\n",
        "    \n",
        "    seq = pad_sequences(seq, maxlen = seq_len)\n",
        "    y_pred = model.predict_classes([seq], verbose  = 0)\n",
        "    word = index2word[y_pred[0]]\n",
        "\n",
        "    input_text.append(word)\n",
        "\t\n",
        "  return ' '.join(input_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlrjguQuO4Jk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate some sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDEVvItsx-KO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZdeLZtvFJHG",
        "colab_type": "code",
        "outputId": "ed41a9dc-4dbc-4e61-9447-36265662eaa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "seed_text = 'how'\n",
        "generate_words(model, tokenizer, seq_len, seed_text, 10, vocab_size) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'how can i do this <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqWQ7Ov3O8Cz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MKE-zl4yhk6",
        "colab_type": "code",
        "outputId": "3d97a6d9-4a36-489e-b005-f2b33faba67a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "seed_text = 'I'\n",
        "generate_words(model, tokenizer, seq_len, seed_text, 10, vocab_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I ' m not sure what you mean by <UNK> <UNK>\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_WgFwlMPMtY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGwwtJHBy46i",
        "colab_type": "code",
        "outputId": "e49c9703-28a4-4afa-d6fd-8230cb2d47e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "seed_text = 'what is the best'\n",
        "generate_words(model, tokenizer, seq_len, seed_text, 10, vocab_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'what is the best way to do this <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPW2MqivzJKn",
        "colab_type": "code",
        "outputId": "0bf7dc1c-6544-4184-803d-12cabd5ddb19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "seed_text = 'how many'\n",
        "generate_words(model, tokenizer, seq_len, seed_text, 10, vocab_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'how many <UNK> are you <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dv3hSuIzMUP",
        "colab_type": "code",
        "outputId": "68f02b56-e9ca-4821-e164-b52a02c4610b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "seed_text = 'another way to'\n",
        "generate_words(model, tokenizer, seq_len, seed_text, 10, vocab_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'another way to do this <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzTFR74CzggQ",
        "colab_type": "text"
      },
      "source": [
        "----\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoma25NFY4K-",
        "colab_type": "text"
      },
      "source": [
        "Write a function that calculates perplexity of a sentence and apply it to a subset of sentences to evaluate the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTmOfDJVVT3p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TO DO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcssNYN4Y54D",
        "colab_type": "text"
      },
      "source": [
        "Define a validation set, for instance 1000 titles\n",
        "Transform that validation set into sequences of tokens using the training vocabulary\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImVVdn2CY7fY",
        "colab_type": "text"
      },
      "source": [
        "Tune the neural net and the parameters of the pre processing phase to improve perplexity score of the model."
      ]
    }
  ]
}