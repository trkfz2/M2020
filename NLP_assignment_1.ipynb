{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_assignment_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOm1Ny8b5tRJCZKoGQDjLUM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trkfz2/M2020/blob/master/NLP_assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asv4Bn8w2ZbL",
        "colab_type": "text"
      },
      "source": [
        "# **Load and prepare data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OY1r6SU02Uxq",
        "colab_type": "text"
      },
      "source": [
        "## 1. Load the dataset into a pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5IopenDw2CU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWE_R-ZGx-og",
        "colab_type": "code",
        "outputId": "1d62efb4-14ee-4a4b-dd8c-6749ff43d4b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://alexip-ml.s3.amazonaws.com/stackexchange_812k.csv.gz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-02 12:17:31--  https://alexip-ml.s3.amazonaws.com/stackexchange_812k.csv.gz\n",
            "Resolving alexip-ml.s3.amazonaws.com (alexip-ml.s3.amazonaws.com)... 52.216.98.187\n",
            "Connecting to alexip-ml.s3.amazonaws.com (alexip-ml.s3.amazonaws.com)|52.216.98.187|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 130230720 (124M) [application/x-gzip]\n",
            "Saving to: ‘stackexchange_812k.csv.gz’\n",
            "\n",
            "stackexchange_812k. 100%[===================>] 124.20M  33.3MB/s    in 3.9s    \n",
            "\n",
            "2020-03-02 12:17:36 (31.8 MB/s) - ‘stackexchange_812k.csv.gz’ saved [130230720/130230720]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMQu4jfYyUS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gunzip stackexchange_812k.csv.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i34tn_ES1fWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"stackexchange_812k.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUqo4JnK1_x-",
        "colab_type": "code",
        "outputId": "fe0f263b-30a9-4748-de81-560fb5ee45f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(812132, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsGCo9gf2CGF",
        "colab_type": "code",
        "outputId": "d33b26de-a05e-4c2d-bcaa-a4c097ed236d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.columns"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['post_id', 'parent_id', 'comment_id', 'text', 'category'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAIKxE0sHFiF",
        "colab_type": "code",
        "outputId": "77847ddf-5856-4394-f0ea-83b5c1984578",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "post_id         int64\n",
              "parent_id     float64\n",
              "comment_id    float64\n",
              "text           object\n",
              "category       object\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obSx1qrK5vsH",
        "colab_type": "text"
      },
      "source": [
        "## 2. Use regular expressions to remove elements that are not words\n",
        " such as:\n",
        " - html tags, \n",
        " - latex expressions, \n",
        " - urls, \n",
        " - digits, \n",
        " - line returns, …\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cboluMK4B10n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_html_tags(text):\n",
        "    \"\"\"Remove html tags from a string\"\"\"  \n",
        "    clean = re.compile('<.*?>') \n",
        "    return re.sub(clean, '', text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAeuiJ0IJKW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_latex(text):\n",
        "    \"\"\"Remove html tags from a string\"\"\"  \n",
        "    clean = re.compile('\\$.*?\\$') \n",
        "    return re.sub(clean, '', text)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvNeCOZzQV6S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_url(text):\n",
        "    \"\"\"\"Remove urls from a string\"\"\"  \n",
        "    text = re.sub(r'(http\\S+|ftp\\S+|www\\S+)', '', text) \n",
        "    return(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNb3xVikRpnS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_specialchar(text):\n",
        "    \"\"\"\"Remove urls from a string\"\"\"  \n",
        "    text = re.sub('[^A-Za-z,.!? ]+', '', text) #'[^A-Za-z0-9 .!,:?]+'\n",
        "    return(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCglnRlZY-74",
        "colab_type": "text"
      },
      "source": [
        " Remove:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoLCbZ3PY9pq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove html\n",
        "df['text'] = df['text'].apply(lambda x: remove_html_tags(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvTum-_ZZIXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove latex\n",
        "df['text'] = df['text'].apply(lambda x: remove_latex(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KN9vvacmZq-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove url\n",
        "df['text'] = df['text'].apply(lambda x: remove_url(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7e8IIewfZ5Yl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove special characters \n",
        "df['text'] = df['text'].apply(lambda x: remove_specialchar(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORngzY6z50bb",
        "colab_type": "text"
      },
      "source": [
        "## 3. Remove missing values for texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rATo-q49ThPg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJw7WpEuUE51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "na = df['text'].isna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAKvm4X9Ut9f",
        "colab_type": "code",
        "outputId": "58cd7f61-1354-43aa-f2c1-e6457c780417",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sum(na == False) == len(df['text'])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fbl2F26N55bq",
        "colab_type": "text"
      },
      "source": [
        "## 4. Remove texts that are extremely large or too short\n",
        " to bring any information to the model. We want to keep paragraphs that contain at least a few words and remove the paragraphs that are composed of large numerical tables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlcOlQ7xwVKy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_length = df['text'].apply(lambda x: len(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYyD3fq7EbqQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9a048eec-ef3f-461e-f83a-22d50ea460e4"
      },
      "source": [
        "min(text_length), max(text_length)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 21933)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeoXIUT7wLi7",
        "colab_type": "text"
      },
      "source": [
        "Short text (number of characters < 30):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEH5eZdGw_CC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "short_text = df[text_length < 30]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbNDHWk_xKZw",
        "colab_type": "code",
        "outputId": "b4f50daa-199c-43db-ee26-706b2d4a80c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "short_text.shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(27934, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4DFtEU7B1iq",
        "colab_type": "text"
      },
      "source": [
        "Show some examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOV5yCZEjY0L",
        "colab_type": "code",
        "outputId": "fd179153-d1f6-46e8-ed26-4199364687f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df['category'].unique()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['title', 'post', 'comment'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IbXZNYsjFgG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "bb635d77-e7f6-4094-de32-f1b006d497a7"
      },
      "source": [
        "short_text[short_text.category == 'post'][0:10]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_id</th>\n",
              "      <th>parent_id</th>\n",
              "      <th>comment_id</th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>91896</th>\n",
              "      <td>259</td>\n",
              "      <td>258.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Support vector machine</td>\n",
              "      <td>post</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91927</th>\n",
              "      <td>326</td>\n",
              "      <td>114.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>XIANS OG</td>\n",
              "      <td>post</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91985</th>\n",
              "      <td>435</td>\n",
              "      <td>423.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>One more Dilbert cartoon...</td>\n",
              "      <td>post</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92036</th>\n",
              "      <td>546</td>\n",
              "      <td>423.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Another one from xkcd</td>\n",
              "      <td>post</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92105</th>\n",
              "      <td>656</td>\n",
              "      <td>423.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "      <td>post</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92746</th>\n",
              "      <td>1890</td>\n",
              "      <td>1883.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The creation of this site</td>\n",
              "      <td>post</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92761</th>\n",
              "      <td>1920</td>\n",
              "      <td>1904.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ACM SIGKDD KDD  in San Diego</td>\n",
              "      <td>post</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92970</th>\n",
              "      <td>2293</td>\n",
              "      <td>1906.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NIPS</td>\n",
              "      <td>post</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93046</th>\n",
              "      <td>2431</td>\n",
              "      <td>2423.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All of Statistics</td>\n",
              "      <td>post</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93210</th>\n",
              "      <td>2729</td>\n",
              "      <td>2728.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>What about silhouette?</td>\n",
              "      <td>post</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       post_id  parent_id  comment_id                          text category\n",
              "91896      259      258.0         NaN        Support vector machine     post\n",
              "91927      326      114.0         NaN                      XIANS OG     post\n",
              "91985      435      423.0         NaN   One more Dilbert cartoon...     post\n",
              "92036      546      423.0         NaN         Another one from xkcd     post\n",
              "92105      656      423.0         NaN                                   post\n",
              "92746     1890     1883.0         NaN    The creation of this site      post\n",
              "92761     1920     1904.0         NaN  ACM SIGKDD KDD  in San Diego     post\n",
              "92970     2293     1906.0         NaN                        NIPS       post\n",
              "93046     2431     2423.0         NaN             All of Statistics     post\n",
              "93210     2729     2728.0         NaN       What about silhouette?      post"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5D-RJhUkPGO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "8efd86f9-6de1-48de-a124-f70a0af8614a"
      },
      "source": [
        "short_text[short_text.category == 'comment'][0:10]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_id</th>\n",
              "      <th>parent_id</th>\n",
              "      <th>comment_id</th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>259062</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15949.0</td>\n",
              "      <td>A related question</td>\n",
              "      <td>comment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>259076</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>800725.0</td>\n",
              "      <td></td>\n",
              "      <td>comment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>259082</th>\n",
              "      <td>9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2723.0</td>\n",
              "      <td>James</td>\n",
              "      <td>comment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>259099</th>\n",
              "      <td>18</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.0</td>\n",
              "      <td>also the US census data</td>\n",
              "      <td>comment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>259110</th>\n",
              "      <td>26</td>\n",
              "      <td>NaN</td>\n",
              "      <td>127496.0</td>\n",
              "      <td>Here is a good explanation</td>\n",
              "      <td>comment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>259121</th>\n",
              "      <td>30</td>\n",
              "      <td>NaN</td>\n",
              "      <td>245.0</td>\n",
              "      <td>Similar question on SO</td>\n",
              "      <td>comment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>259133</th>\n",
              "      <td>36</td>\n",
              "      <td>NaN</td>\n",
              "      <td>18.0</td>\n",
              "      <td></td>\n",
              "      <td>comment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>259149</th>\n",
              "      <td>43</td>\n",
              "      <td>NaN</td>\n",
              "      <td>206.0</td>\n",
              "      <td>fine, you win</td>\n",
              "      <td>comment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>259151</th>\n",
              "      <td>44</td>\n",
              "      <td>NaN</td>\n",
              "      <td>362884.0</td>\n",
              "      <td>is this your homework?</td>\n",
              "      <td>comment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>259205</th>\n",
              "      <td>75</td>\n",
              "      <td>NaN</td>\n",
              "      <td>85.0</td>\n",
              "      <td></td>\n",
              "      <td>comment</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        post_id  parent_id  comment_id                        text category\n",
              "259062        4        NaN     15949.0         A related question   comment\n",
              "259076        7        NaN    800725.0                              comment\n",
              "259082        9        NaN      2723.0                  James       comment\n",
              "259099       18        NaN         7.0    also the US census data   comment\n",
              "259110       26        NaN    127496.0  Here is a good explanation  comment\n",
              "259121       30        NaN       245.0     Similar question on SO   comment\n",
              "259133       36        NaN        18.0                              comment\n",
              "259149       43        NaN       206.0              fine, you win   comment\n",
              "259151       44        NaN    362884.0      is this your homework?  comment\n",
              "259205       75        NaN        85.0                              comment"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y57hwqsWkTFv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "e8646ebe-de80-4a59-9278-57c45663690b"
      },
      "source": [
        "short_text[short_text.category == 'title'][0:10]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_id</th>\n",
              "      <th>parent_id</th>\n",
              "      <th>comment_id</th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eliciting priors from experts</td>\n",
              "      <td>title</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>What is normality?</td>\n",
              "      <td>title</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>23</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Finding the PDF given the CDF</td>\n",
              "      <td>title</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>26</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>What is a standard deviation?</td>\n",
              "      <td>title</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>44</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Explain data visualization</td>\n",
              "      <td>title</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>138</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Free resources for learning R</td>\n",
              "      <td>title</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>145</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Free Dataset Resources?</td>\n",
              "      <td>title</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>170</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Free statistical textbooks</td>\n",
              "      <td>title</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>249</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Variance components</td>\n",
              "      <td>title</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>290</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Resources for learning Stata</td>\n",
              "      <td>title</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    post_id  parent_id  comment_id                           text category\n",
              "0         1        NaN         NaN  Eliciting priors from experts    title\n",
              "1         2        NaN         NaN             What is normality?    title\n",
              "11       23        NaN         NaN  Finding the PDF given the CDF    title\n",
              "13       26        NaN         NaN  What is a standard deviation?    title\n",
              "19       44        NaN         NaN     Explain data visualization    title\n",
              "37      138        NaN         NaN  Free resources for learning R    title\n",
              "38      145        NaN         NaN        Free Dataset Resources?    title\n",
              "42      170        NaN         NaN     Free statistical textbooks    title\n",
              "57      249        NaN         NaN            Variance components    title\n",
              "63      290        NaN         NaN   Resources for learning Stata    title"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PXuU3KTB57R",
        "colab_type": "text"
      },
      "source": [
        "Remove short text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-ijTNIJ1QZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df[~df.index.isin(short_text.index)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohiAmqdA3M1I",
        "colab_type": "code",
        "outputId": "025f726c-b7aa-4b18-dbfb-5186356a6864",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(784198, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRoW3kf3JrPf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "750a00c9-0f10-4849-c60b-26ec018043cf"
      },
      "source": [
        "new_length = df['text'].apply(lambda x: len(x))\n",
        "min(new_length), max(new_length)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30, 21933)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXRiWsit3ULd",
        "colab_type": "text"
      },
      "source": [
        "Long text (number of characters > 10000):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YW2OnrT33U4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "long_text = df[text_length > 10000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-dyjF2g4NwJ",
        "colab_type": "code",
        "outputId": "212db0b9-4403-4313-9836-94a57d975538",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "long_text.shape"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(143, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS6P-qDbHNU3",
        "colab_type": "text"
      },
      "source": [
        "Show some examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwOfIdHtHGj7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.options.display.max_colwidth = max(text_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRrB0cwuGtfr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ba666f48-5f9d-490b-8f68-5fd45e36e02b"
      },
      "source": [
        "long_text[long_text.category == 'post'][0:2]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_id</th>\n",
              "      <th>parent_id</th>\n",
              "      <th>comment_id</th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>92968</th>\n",
              "      <td>2287</td>\n",
              "      <td>2272.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I agree completely with Srikants explanation. To give a more heuristic spin on itClassical approaches generally posit that the world is one way e.g., a parameter has one particular true value, and try to conduct experiments whose resulting conclusion  no matter the true value of the parameter  will be correct with at least some minimum probability.As a result, to express uncertainty in our knowledge after an experiment, the frequentist approach uses a confidence interval  a range of values designed to include the true value of the parameter with some minimum probability, say . A frequentist will design the experiment and  confidence interval procedure so that out of every  experiments run start to finish, at least  of the resulting confidence intervals will be expected to include the true value of the parameter. The other  might be slightly wrong, or they might be complete nonsense  formally speaking thats ok as far as the approach is concerned, as long as  out of  inferences are correct. Of course we would prefer them to be slightly wrong, not total nonsense.Bayesian approaches formulate the problem differently. Instead of saying the parameter simply has one unknown true value, a Bayesian method says the parameters value is fixed but has been chosen from some probability distribution  known as the prior probability distribution. Another way to say that is that before taking any measurements, the Bayesian assigns a probability distribution, which they call a belief state, on what the true value of the parameter happens to be. This prior might be known imagine trying to estimate the size of a truck, if we know the overall distribution of truck sizes from the DMV or it might be an assumption drawn out of thin air. The Bayesian inference is simpler  we collect some data, and then calculate the probability of different values of the parameter GIVEN the data. This new probability distribution is called the a posteriori probability or simply the posterior. Bayesian approaches can summarize their uncertainty by giving a range of values on the posterior probability distribution that includes  of the probability  this is called a  credibility interval.A Bayesian partisan might criticize the frequentist confidence interval like this So what if  out of  experiments yield a confidence interval that includes the true value? I dont care about  experiments I DIDNT DO I care about this experiment I DID DO. Your rule allows  out of the  to be complete nonsense negative values, impossible values as long as the other  are correct thats ridiculous.A frequentist diehard might criticize the Bayesian credibility interval like this So what if  of the posterior probability is included in this range? What if the true value is, say, .? If it is, then your method, run start to finish, will be WRONG  of the time. Your response is, Oh well, thats ok because according to the prior its very rare that the value is ., and that may be so, but I want a method that works for ANY possible value of the parameter. I dont care about  values of the parameter that IT DOESNT HAVE I care about the one true value IT DOES HAVE. Oh also, by the way, your answers are only correct if the prior is correct. If you just pull it out of thin air because it feels right, you can be way off.In a sense both of these partisans are correct in their criticisms of each others methods, but I would urge you to think mathematically about the distinction  as Srikant explains.Heres an extended example from that talk that shows the difference precisely in a discrete example.When I was a child my mother used to occasionally surprise me by ordering a jar of chocolatechip cookies to be delivered by mail. The delivery company stocked four different kinds of cookie jars  type A, type B, type C, and type D, and they were all on the same truck and you were never sure what type you would get. Each jar had exactly  cookies, but the feature that distinguished the different cookie jars was their respective distributions of chocolate chips per cookie. If you reached into a jar and took out a single cookie uniformly at random, these are the probability distributions you would get on the number of chipsA typeA cookie jar, for example, has  cookies with two chips each, and no cookies with four chips or more! A typeD cookie jar has  cookies with one chip each. Notice how each vertical column is a probability mass function  the conditional probability of the number of chips youd get, given that the jar  A, or B, or C, or D, and each column sums to .I used to love to play a game as soon as the deliveryman dropped off my new cookie jar. Id pull one single cookie at random from the jar, count the chips on the cookie, and try to express my uncertainty  at the  level  of which jars it could be. Thus its the identity of the jar A, B, C or D that is the value of the parameter being estimated. The number of chips , , ,  or  is the outcome or the observation or the sample.Originally I played this game using a frequentist,  confidence interval. Such an interval needs to make sure that no matter the true value of the parameter, meaning no matter which cookie jar I got, the interval would cover that true value with at least  probability.An interval, of course, is a function that relates an outcome a row to a set of values of the parameter a set of columns. But to construct the confidence interval and guarantee  coverage, we need to work vertically  looking at each column in turn, and making sure that  of the probability mass function is covered so that  of the time, that columns identity will be part of the interval that results. Remember that its the vertical columns that form a p.m.f.So after doing that procedure, I ended up with these intervalsFor example, if the number of chips on the cookie I draw is , my confidence interval will be B,C,D. If the number is , my confidence interval will be B,C. Notice that since each column sums to  or greater, then no matter which column we are truly in no matter which jar the deliveryman dropped off, the interval resulting from this procedure will include the correct jar with at least  probability.Notice also that the procedure I followed in constructing the intervals had some discretion. In the column for typeB, I could have just as easily made sure that the intervals that included B would be ,,, instead of ,,,. That would have resulted in  coverage for typeB jars , still meeting the lower bound of .My sister Bayesia thought this approach was crazy, though. You have to consider the deliverman as part of the system, she said. Lets treat the identity of the jar as a random variable itself, and lets assume that the deliverman chooses among them uniformly  meaning he has all four on his truck, and when he gets to our house he picks one at random, each with uniform probability.With that assumption, now lets look at the joint probabilities of the whole event  the jar type and the number of chips you draw from your first cookie, she said, drawing the following tableNotice that the whole table is now a probability mass function  meaning the whole table sums to .Ok, I said, where are you headed with this?Youve been looking at the conditional probability of the number of chips, given the jar, said Bayesia. Thats all wrong! What you really care about is the conditional probability of which jar it is, given the number of chips on the cookie! Your  interval should simply include the list jars that, in total, have  probability of being the true jar. Isnt that a lot simpler and more intuitive?Sure, but how do we calculate that? I asked.Lets say we know that you got  chips. Then we can ignore all the other rows in the table, and simply treat that row as a probability mass function. Well need to scale up the probabilities proportionately so each row sums to , though. She didNotice how each row is now a p.m.f., and sums to . Weve flipped the conditional probability from what you started with  now its the probability of the man having dropped off a certain jar, given the number of chips on the first cookie.Interesting, I said. So now we just circle enough jars in each row to get up to  probability? We did just that, making these credibility intervalsEach interval includes a set of jars that, a posteriori, sum to  probability of being the true jar.Well, hang on, I said. Im not convinced. Lets put the two kinds of intervals sidebyside and compare them for coverage and, assuming that the deliveryman picks each kind of jar with equal probability, credibility.Here they areConfidence intervalsCredibility intervalsSee how crazy your confidence intervals are? said Bayesia. You dont even have a sensible answer when you draw a cookie with zero chips! You just say its the empty interval. But thats obviously wrong  it has to be one of the four types of jars. How can you live with yourself, stating an interval at the end of the day when you know the interval is wrong? And ditto when you pull a cookie with  chips  your interval is only correct  of the time. Calling this a  confidence interval is bullshit.Well, hey, I replied. Its correct  of the time, no matter which jar the deliveryman dropped off. Thats a lot more than you can say about your credibility intervals. What if the jar is type B? Then your interval will be wrong  of the time, and only correct  of the time!This seems like a big problem, I continued, because your mistakes will be correlated with the type of jar. If you send out  Bayesian robots to assess what type of jar you have, each robot sampling one cookie, youre telling me that on typeB days, you will expect  of the robots to get the wrong answer, each having  belief in its incorrect conclusion! Thats troublesome, especially if you want most of the robots to agree on the right answer.PLUS we had to make this assumption that the deliveryman behaves uniformly and selects each type of jar at random, I said. Where did that come from? What if its wrong? You havent talked to him you havent interviewed him. Yet all your statements of a posteriori probability rest on this statement about his behavior. I didnt have to make any such assumptions, and my interval meets its criterion even in the worst case.Its true that my credibility interval does perform poorly on typeB jars, Bayesia said. But so what? Type B jars happen only  of the time. Its balanced out by my good coverage of type A, C, and D jars. And I never publish nonsense.Its true that my confidence interval does perform poorly when Ive drawn a cookie with zero chips, I said. But so what? Chipless cookies happen, at most,  of the time in the worst case a typeD jar. I can afford to give nonsense for this outcome because NO jar will result in a wrong answer more than  of the time.The column sums matter, I said.The row sums matter, Bayesia said.I can see were at an impasse, I said. Were both correct in the mathematical statements were making, but we disagree about the appropriate way to quantify uncertainty.Thats true, said my sister. Want a cookie?</td>\n",
              "      <td>post</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93715</th>\n",
              "      <td>3673</td>\n",
              "      <td>3419.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>You asked a difficult question, but Im a little bit surprised that the various clues that were suggested to you received so little attention. I upvoted all of them because I think they basically are useful responses, though in their actual form they call for further bibliographic work.Disclaimer I never had to deal with such a problem, but I regularly have to expose statistical results that may differ from physicians a priori beliefs and I learn a lot from unraveling their lines of reasoning. Also, I have some background in teaching human decisionknowledge from the perspective of Artificial Intelligence and Cognitive Science, and I think what you asked is not so far from how experts actually decide that two objects are similar or not, based on their attributes and a common understanding of their relationships.From your question, I noticed two interesting assertions. The first one related to how an expert assess the similarity or difference between two set of measurements  I dont particularly care if there is  some relation between attribute X and  Y. What I care about is if a doctor  thinks there is a relation between X  and Y.The second one,   How can I predict what they think the  similarity is? Do they look at certain  attributes?looks like it is somewhat subsumed by the former, but it seems more closely related to what are the most salient attributes that allow to draw a clear separation between the objects of interest.To the first question, I would answer Well, if there is no characteristic or objective relationship between any two subjects, what would be the rationale for making up an hypothetical one? Rather, I think the question should be If I only have limited resources knowledge, time, data to take a decision, how do I optimize my choice? To the second question, my answer is Although it seems to partly contradicts your former assertion if there is no relationship at all, it implies that the available attributes are not discriminative or useless, I think that most of the time this is a combination of attributes that makes sense, and not only how a given individual scores on a single attribute.Let me dwell on these two points.Human beings have a limited or bounded rationality, and can take a decision often the right one without examining all possible solutions. There is also a close connection with abductive reasoning.It is well known that there is some variability between individual judgments, and even between judgments from the same expert at two occasions. This is what we are interested in in reliability studies. But you want to know how these experts elaborate their judgments. There is a huge amount of papers about that in cognitive psychology, especially on the fact that relative judgments are easier and more reliable than absolute ones. Doctors decisions are interesting in this respect because they are able to take a good decision with a limited amount of information, but at the same time they benefit from an ever growing internal knowledge base from which they can draw expected relationships extrapolation. In other words, they have a builtin inference assumed to be hypotheticodeductive machinery and accumulate positive evidence or counterfactuals from there experience or practice. Reproducing this inferential ability and the use of declarative knowledge was the aim of several expert or production rule systems in the s, the most famous one being MYCIN, and more generally of Artifical Intelligence earlier in  Can we reproduce on an artificial system the intelligent behavior observed in man?. Automatic treatment of speech, problem solving, visual shape recognition are still active projects nowadays and they all have to do with identifying salient features and their relationships to make an appropriate decision i.e., how far should two patterns be to be judged as the emanation of two distinct generating processes?.In sum, our doctors are able to draw an optimal inference from a limited amount of data, compensating from noise that arises simply as a byproduct of individual variability at the level of the patients. Thus, there is a clear connection with statistics and probability theory, and the question is what conscious or subconscious methodology help doctors forming their judgments. Semantic networks SN, belief networks, and decision trees are all relevant to the question you asked. The paper you cited is about using an ontology as a basis of formal judgments, but it is no more than an extension of SNs, and many projects were initiated in this direction I can think of the Gene Ontology for genomic studies, but many others exist in different domains. Now, look at the following hierarchical classification of diagnostic categories it is roughly taken from Dunn , p. And now take a look at the ICD classification I think it is not too far from this schematic classification. Mental disorders are organized into distinct categories, some of them being closer one to each other. What render them similar is the closeness of their expression phenotype in any patient, and the fact that they share some similarities in their somaticpsychological etiology. Assessing whether two doctors would make the same diagnostic is a typical example of an interrater agreement study, where two psychiatrists are asked to place each of several patients in mutually exclusive categories. The hierarchical structure should be reflected in the disagreement between each doctor, that is they may not agree on the finer distinction between diagnostic classes the leafs but if they were to disagree between insomnia and schizophrenia, well it would be little bit disconcerting... How these two doctors decide on which class a given patient belongs to is no more than a clustering problem How likely are two individuals, given a set of observed values on different attributes, to be similar enough so that I decide they share the same class membership? Now, some attributes are more influential than others, and this is exactly what is reflected in the weight attributed to a given attribute in Latent Class Analysis which can be thought of as a probabilistic extension of clustering methods like kmeans, or the variable importance in Random Forests. We need to put things into boxes, because at first sight its simpler. The problem is that often things overlap to some extent, so we need to consider different levels of categorization.In fact, cluster analysis is at the heart of the actual DSM categories, and many papers actually turn around assigning one patient to a specific syndromic category, based on the profile of his response to a battery of neuropsychological assessments. This merely looks like a subtyping approach each time, we seek to refine a preliminary wellestablished diagnostic category, by adding exception rules or an additional relevant symptom or impairment. A related topic is decision trees which are by far the most well understood statistical techniques by physicians. Most of the time, they described a nested series of boolean assertions Do you have a sore throat? If yes, do you have a temperature? etc. but look at an example of public influenza diagnostic tree according to which we can form a decision regarding patients proximity i.e. how similar patients are wrt. attributes considered for building the tree  the closer they are the more likely they are to end up in the same leaf. Association rules and the  C. algorithm rely quite on the same idea. On a related topic, theres the patient ruleinduction method PRIM. Now clearly, we must make a distinction between all those methods, that make an efficient use of a large body of data and incorporate bagging or boosting to compensate for model fragility or overfitting issues, and doctors who cannot process huge amount of data in an automatic and algorithmic manner. But, for small to moderate amount of descriptors, I think they perform quite good after all.The yesorno approach is not the panacea, though. In behavioral genetics and psychiatry, it is commonly argued that the classification approach is probably not the best way to go, and that common diseases learning disorders, depression, personality disorders, etc. reflect a continuum rather than classes of opposite valence. Nobodys perfect!In conclusion, I think doctors actually hold a kind of internalized inference engine that allows them to assign patients into distinctive classes that are characterized by a weighted combination of available evidences in other words, they are able to organize their knowledge in an efficient manner, and these internal representations and the relationships they share may be augmented throughout experience. Casebased reasoning probably comes into play at some point too. All of this may be subjected to a revision with newly available data we are not simply acting as definitive binary classifiers, and are able to incorporate new data in our decision making, and b subjective biases arising from past experience or wrong selfmade association rules. However, they are prone to errors, as every decision systems...All statistical techniques reflecting these steps  decisions trees, baggingboosting, cluster analysis, latent cluster analysis  seems relevant to your questions, although they may be hard to instantiate in a single decision rule. Here are a couple of references that might be helpful, as a first start to how doctors make their decisionsA clinical decision support system for clinicians for determining appropriate radiologic imaging examinationGrzymalaBusse, JW. Selected Algorithms of Machine Learning from Examples. Fundamenta Informaticae  , Santiago Medina, L, Kuntz, KM, and Pomeroy, S. Children With Headache Suspected of Having a Brain Tumor A CostEffectiveness Analysis of Diagnostic Strategies. Pediatrics  , Building Better Algorithms for the Diagnosis of Nontraumatic HeadacheJenkins, J, Shields, M, Patterson, C, and Kee, F. Decision making in asthma exacerbation a clinical judgement analysis. Arch Dis Child  , Croskerry, P. Achieving quality in clinical decision making cognitive strategies and detection of bias. Acad Emerg Med  , .Cahan, A, Gilon, D, Manor, O, and Paltiel. Probabilistic reasoning and clinical decisionmaking do doctors overestimate diagnostic probabilities? QJM  , Wegwarth, O, Gaissmaier, W, and Gigerenzer, G. Smart strategies for doctors and doctorsintraining heuristics in medicine. Medical Education  ,</td>\n",
              "      <td>post</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       post_id  ...  category\n",
              "92968     2287  ...      post\n",
              "93715     3673  ...      post\n",
              "\n",
              "[2 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "122FZtw9HFGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.options.display.max_colwidth = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gRucguX3-sU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df[~df.index.isin(long_text.index)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EO7AL8E4YhY",
        "colab_type": "code",
        "outputId": "1b0e8026-66b9-4a76-944a-4f535026a587",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(784055, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwGiRXUg4b1j",
        "colab_type": "code",
        "outputId": "7338ff7e-e2f8-493d-8c94-e596c54ac399",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "df.iloc[0], df.iloc[45457], df.iloc[-1]"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(post_id                                                       3\n",
              " parent_id                                                   NaN\n",
              " comment_id                                                  NaN\n",
              " text          What are some valuable Statistical Analysis op...\n",
              " category                                                  title\n",
              " Name: 2, dtype: object,\n",
              " post_id                                                  288819\n",
              " parent_id                                                   NaN\n",
              " comment_id                                                  NaN\n",
              " text          Using categorical variable in Conditional Logi...\n",
              " category                                                  title\n",
              " Name: 49662, dtype: object,\n",
              " post_id                                                  279999\n",
              " parent_id                                                   NaN\n",
              " comment_id                                               542550\n",
              " text          As per your other question, your data does not...\n",
              " category                                                comment\n",
              " Name: 812131, dtype: object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FO0zPwNYInJY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "76db5248-9384-432f-ebb9-9389e0b1ba63"
      },
      "source": [
        "new_length = df['text'].apply(lambda x: len(x))\n",
        "min(new_length), max(new_length)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30, 9998)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qa6xBSI12Qmv",
        "colab_type": "text"
      },
      "source": [
        "## 5. Use a tokenizer\n",
        " to create a version of the original text that is a string of space-separated lowercase tokens. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdMC6v-o62uY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import WordPunctTokenizer \n",
        "     \n",
        "# Create a reference variable for Class WordPunctTokenizer \n",
        "tk = WordPunctTokenizer() \n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: tk.tokenize(x)) \n",
        "     \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RE2apxQP73rW",
        "colab_type": "code",
        "outputId": "33df0bae-3af0-4cdd-d520-cb0260e46d25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "print(df.iloc[0])\n",
        "print('---')\n",
        "print(df.iloc[456785])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "post_id                                                       3\n",
            "parent_id                                                   NaN\n",
            "comment_id                                                  NaN\n",
            "text          [What, are, some, valuable, Statistical, Analy...\n",
            "category                                                  title\n",
            "Name: 2, dtype: object\n",
            "---\n",
            "post_id                                                  362481\n",
            "parent_id                                                   NaN\n",
            "comment_id                                               680911\n",
            "text          [This, is, possibly, better, asked, on, crossv...\n",
            "category                                                comment\n",
            "Name: 472449, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p38BibF35_9R",
        "colab_type": "text"
      },
      "source": [
        "## 6. Export the resulting dataframe into a csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_-nY12D82u_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv('tokenized_text.csv', index = True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}