{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_assignment_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMThc8jB02jc+sMTA+tymlF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trkfz2/M2020/blob/master/NLP_assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZyillwDbQov",
        "colab_type": "code",
        "outputId": "ffdada18-659a-4752-d250-d4d675ad7155",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "!pip install --upgrade nltk"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-cp36-none-any.whl size=1449905 sha256=129e8c2624fd051f4f8983f70bb8de8f95012935289d831f1a04abbbfdb19b63\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJaqo6G_bblZ",
        "colab_type": "code",
        "outputId": "8b214e18-d42a-4af6-f71b-9d6fca57a4b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import nltk\n",
        "nltk.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.4.5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0JCTDMashzV",
        "colab_type": "code",
        "outputId": "a1c30ef3-109d-44af-9a38-1ecde8b585a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "nltk.download()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> punkt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "    Downloading package punkt to /root/nltk_data...\n",
            "      Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mn5rcn6xxs8I",
        "colab_type": "text"
      },
      "source": [
        "# 1. Split the dataset into a training and a testing subset\n",
        "\n",
        "Use the category “title” as the testing set and the categories “comment” and “post” as the training set. The short length of titles will make them good candidates later on as seeds for text generation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnbHFvjWyRwC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU0aRKXW1-4L",
        "colab_type": "text"
      },
      "source": [
        "### Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SDbKN5G1lpz",
        "colab_type": "code",
        "outputId": "6cf5edb0-b676-4ca9-d324-f28ee013ff32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RK1ZD3AyV65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('drive/My Drive/Manning Project/NLP/stackexchange_812k.tokenized.csv').sample(frac = 1, random_state = 0).reset_index(drop = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnrjIP4h5T7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert data.shape == (789649, 7), \"The dataset does not have the right dimensions\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ainzreA6--a",
        "colab_type": "code",
        "outputId": "1de5e974-cea3-4ff6-9b98-38d03dff53e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "data.columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['post_id', 'parent_id', 'comment_id', 'text', 'category', 'tokens',\n",
              "       'n_tokens'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9YrqxTV8ZIc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.options.display.max_colwidth = 500"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu7xB2fw8No1",
        "colab_type": "code",
        "outputId": "f3a031d5-2ac4-41c9-e404-438d2e5c7bde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_id</th>\n",
              "      <th>parent_id</th>\n",
              "      <th>comment_id</th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "      <th>tokens</th>\n",
              "      <th>n_tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>298828</td>\n",
              "      <td>NaN</td>\n",
              "      <td>567970.0</td>\n",
              "      <td>That's not an option atm. This data definitely provides some evidence for length of adherence, I just don't know how to use it.</td>\n",
              "      <td>comment</td>\n",
              "      <td>that ' s not an option atm . this data definitely provides some evidence for length of adherence , i just don ' t know how to use it .</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>142749</td>\n",
              "      <td>142741.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>already provided a link to the discussion containing the theoretical aspects. Here is a quick pratical example of how one would do it in R. Please also have a look at these documents which contain the theory as well as examples Simultaneous Inference in General Parametric Models and Additional multcomp Examples . We will use the mtcars dataset and build a linear regression model containing three variables cyl Number of cylinders , disp Displacement and hp Horsepower to predict the variable m...</td>\n",
              "      <td>post</td>\n",
              "      <td>already provided a link to the discussion containing the theoretical aspects . here is a quick pratical example of how one would do it in r . please also have a look at these documents which contain the theory as well as examples simultaneous inference in general parametric models and additional multcomp examples . we will use the mtcars dataset and build a linear regression model containing three variables cyl number of cylinders , disp displacement and hp horsepower to predict the variable...</td>\n",
              "      <td>332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>49674</td>\n",
              "      <td>NaN</td>\n",
              "      <td>96088.0</td>\n",
              "      <td>As for I would say this question is off-topic and unlikely to help future visitors interested in statistical science in its theoretical and applied aspects.</td>\n",
              "      <td>comment</td>\n",
              "      <td>as for i would say this question is off - topic and unlikely to help future visitors interested in statistical science in its theoretical and applied aspects .</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>264715</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I attended a conference on ML and Data Science and I have a general question that was not answered in the conference. If we have a continuous variable, let's say age. What is the best way to handle this variable. These are my thoughts, please let me know if they nonsense, but in general I think it is a very important and useful topic that has not been discussed in the detail that I need it How should you decide on the number of bins? Would it be best to choose an arbitrary number of bins and...</td>\n",
              "      <td>post</td>\n",
              "      <td>i attended a conference on ml and data science and i have a general question that was not answered in the conference . if we have a continuous variable , let ' s say age . what is the best way to handle this variable . these are my thoughts , please let me know if they nonsense , but in general i think it is a very important and useful topic that has not been discussed in the detail that i need it how should you decide on the number of bins ? would it be best to choose an arbitrary number of...</td>\n",
              "      <td>258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>45344</td>\n",
              "      <td>NaN</td>\n",
              "      <td>88126.0</td>\n",
              "      <td>Oh well, I actually meant equivalence in terms of only observed variables -- essentially maxing or more commonly, marginalizing out latent variables and then establishing equivalence.</td>\n",
              "      <td>comment</td>\n",
              "      <td>oh well , i actually meant equivalence in terms of only observed variables -- essentially maxing or more commonly , marginalizing out latent variables and then establishing equivalence .</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   post_id  ...  n_tokens\n",
              "0   298828  ...        30\n",
              "1   142749  ...       332\n",
              "2    49674  ...        28\n",
              "3   264715  ...       258\n",
              "4    45344  ...        29\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5iqF5OnEyU8",
        "colab_type": "code",
        "outputId": "406f0000-8bae-4b83-bb5f-752798b35a14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "list(data['category'].unique())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['comment', 'post', 'title']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNe54TqB2NQ0",
        "colab_type": "text"
      },
      "source": [
        "Training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZO28MLS6sfy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training = data[data.category.isin([\"post\",\"comment\"]) ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrwyfGVT7Q-8",
        "colab_type": "code",
        "outputId": "1d3ea068-c40d-4769-a99e-310903d91888",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "training.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(705964, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtfXrHl4ypHE",
        "colab_type": "code",
        "outputId": "600a89f3-f06a-437e-95a0-30c1935b1cd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "training.category.value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "comment    540587\n",
              "post       165377\n",
              "Name: category, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-prXYzLT2UhF",
        "colab_type": "text"
      },
      "source": [
        "Testing set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWmrd2XVERh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testing = data[data.category == 'title']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlKHc8QmE48W",
        "colab_type": "code",
        "outputId": "f786a3e2-52d3-420f-f4d8-1f71dbb13136",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "testing.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(83685, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boiMbQEOxyaQ",
        "colab_type": "text"
      },
      "source": [
        "# 2. Build the matrix of prefix - word frequencies\n",
        "\n",
        "Use the ngrams function from nltk.utils to generate all n-grams from the corpus\n",
        "Set the following left_pad_symbol = `<s>` and right_pad_symbol = `</s>`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DsWpLaUznBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we need only tokens for training\n",
        "\n",
        "training_text = training[\"tokens\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D5jkglnzzu4",
        "colab_type": "code",
        "outputId": "406a88b0-9365-48a5-81eb-b62c7fc00fe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "training_text.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(705964,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXJnOZJv0EGG",
        "colab_type": "code",
        "outputId": "85cbf593-f0cc-47d6-abef-571c3a0698a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# shown an example\n",
        "training_text[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"that ' s not an option atm . this data definitely provides some evidence for length of adherence , i just don ' t know how to use it .\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCjTtoBn-us_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import tokenize\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nB0VlF9AE8oV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.util import pad_sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lk2I_N7uCEHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_sent(text):\n",
        "\n",
        "  # split text into sentences\n",
        "  sent = tokenize.sent_tokenize(text)\n",
        "\n",
        "  # split into tokens, pad sentences for bigrams\n",
        "  pad_sent = [list(pad_sequence(item.split(), n = 2, pad_left = True, pad_right = True, left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\")) for item in sent]\n",
        "  \n",
        "  return pad_sent\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af1Gu4Voi9Cf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split comments and posts into sentences, pad both ends\n",
        "\n",
        "pad_tr_text = [prepare_sent(item) for item in training_text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zZA-rcywCCS",
        "colab_type": "code",
        "outputId": "5a5ad292-6cdc-4c88-a2ff-1898d802b7f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "# show one post/comment\n",
        "\n",
        "pad_tr_text[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['<s>', 'that', \"'\", 's', 'not', 'an', 'option', 'atm', '.', '</s>'],\n",
              " ['<s>',\n",
              "  'this',\n",
              "  'data',\n",
              "  'definitely',\n",
              "  'provides',\n",
              "  'some',\n",
              "  'evidence',\n",
              "  'for',\n",
              "  'length',\n",
              "  'of',\n",
              "  'adherence',\n",
              "  ',',\n",
              "  'i',\n",
              "  'just',\n",
              "  'don',\n",
              "  \"'\",\n",
              "  't',\n",
              "  'know',\n",
              "  'how',\n",
              "  'to',\n",
              "  'use',\n",
              "  'it',\n",
              "  '.',\n",
              "  '</s>']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGWifkdyliZv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.lm.preprocessing import flatten"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yozpmy-3lf73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# flatten rows\n",
        "\n",
        "flat_pad_rows = list(flatten(pad_tr_text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNOoBN3Wl-XQ",
        "colab_type": "code",
        "outputId": "bc28c441-8e5d-4986-db13-9e1ad197adf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "# all sentences from the first post/comment\n",
        "\n",
        "flat_pad_rows[0:2]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['<s>', 'that', \"'\", 's', 'not', 'an', 'option', 'atm', '.', '</s>'],\n",
              " ['<s>',\n",
              "  'this',\n",
              "  'data',\n",
              "  'definitely',\n",
              "  'provides',\n",
              "  'some',\n",
              "  'evidence',\n",
              "  'for',\n",
              "  'length',\n",
              "  'of',\n",
              "  'adherence',\n",
              "  ',',\n",
              "  'i',\n",
              "  'just',\n",
              "  'don',\n",
              "  \"'\",\n",
              "  't',\n",
              "  'know',\n",
              "  'how',\n",
              "  'to',\n",
              "  'use',\n",
              "  'it',\n",
              "  '.',\n",
              "  '</s>']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wroePRJ4mjmV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# flatten all sentences into a list of words\n",
        "\n",
        "flat_pad_sent = list(flatten(flat_pad_rows))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyEgPf2-8wey",
        "colab_type": "code",
        "outputId": "9a6ed046-cb92-4579-a464-ddb06896650e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(flat_pad_sent)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54058369"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iQKGhKF80Gv",
        "colab_type": "code",
        "outputId": "fb9b3ca4-fd30-4750-9aa8-27f8f5b39453",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "flat_pad_sent[0:20]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>',\n",
              " 'that',\n",
              " \"'\",\n",
              " 's',\n",
              " 'not',\n",
              " 'an',\n",
              " 'option',\n",
              " 'atm',\n",
              " '.',\n",
              " '</s>',\n",
              " '<s>',\n",
              " 'this',\n",
              " 'data',\n",
              " 'definitely',\n",
              " 'provides',\n",
              " 'some',\n",
              " 'evidence',\n",
              " 'for',\n",
              " 'length',\n",
              " 'of']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmCPSjpc9bNk",
        "colab_type": "text"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PubsgDxZ0UZ_",
        "colab_type": "text"
      },
      "source": [
        "Find all bigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dyYQPqQeGbH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.util import bigrams"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIkZtAjdeOGX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "padded_bigrams = list(bigrams(flat_pad_sent))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GCXo5tUe_Ey",
        "colab_type": "code",
        "outputId": "15322301-aed1-423f-d208-3b38f57c54a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(padded_bigrams)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54058368"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haPxZTz_fVfa",
        "colab_type": "code",
        "outputId": "c308bdf6-6b98-4542-e9fc-b9640bc58fdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# show one bigram\n",
        "\n",
        "padded_bigrams[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('that', \"'\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0Vnttwu933R",
        "colab_type": "text"
      },
      "source": [
        "-----\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmKv3yOX0Xmd",
        "colab_type": "text"
      },
      "source": [
        "N-gram models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y1KolzHW1Zx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter, defaultdict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5mOCeEcom3I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# unigram model\n",
        "\n",
        "model_uni = defaultdict(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8zW_HEuoxe6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# unigram counts\n",
        "\n",
        "for unigram in flat_pad_sent:\n",
        "  model_uni[unigram] +=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1zbFwZBpGCt",
        "colab_type": "code",
        "outputId": "e3282060-8d38-42de-f00d-7e98dc5e2043",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(model_uni)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "141969"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXXmjEjKpNa8",
        "colab_type": "code",
        "outputId": "451040b4-2620-463a-fe7b-6ab102ad9193",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model_uni['that']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "584957"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFe_CpHLpRnW",
        "colab_type": "code",
        "outputId": "553da1f7-8005-4235-d797-141df751326c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model_uni['because']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55682"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcPnHNXl-j07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# bigram model\n",
        "\n",
        "model_bigram = defaultdict(lambda: 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ihOuYhkhdOY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# bigram counts\n",
        "for bigram in padded_bigrams:\n",
        "  model_bigram[(bigram[0], bigram[1])] +=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZYiGLyYkeB6",
        "colab_type": "code",
        "outputId": "5d2857fa-f968-49d5-9fe6-b28a273943c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(model_bigram)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3314199"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78_4rFwp0wkm",
        "colab_type": "text"
      },
      "source": [
        "Log probabilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFBLNXLmq6Jn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0kxN8rXvNfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# bigram log probs\n",
        "\n",
        "model_bigram_prob = defaultdict()\n",
        "\n",
        "for bigram, count in model_bigram.items():\n",
        "  if type(bigram) == int:\n",
        "    continue\n",
        "  \n",
        "  uni_count = model_uni[bigram[0]]\n",
        "  if uni_count == 0:\n",
        "    continue\n",
        "  \n",
        "  prob = count/uni_count\n",
        "  log_prob = math.log(prob)\n",
        "  \n",
        "  model_bigram_prob[bigram] = log_prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdGAXxJKjsrH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# bigram log probs (laplace smoothing)\n",
        "\n",
        "model_bigram_prob_smooth = defaultdict()\n",
        "\n",
        "for bigram, count in model_bigram.items():\n",
        "\n",
        "  if type(bigram) == int:\n",
        "    continue\n",
        "  \n",
        "  uni_count = model_uni[bigram[0]]\n",
        "  if uni_count == 0:\n",
        "    continue\n",
        "  \n",
        "  # laplace smoothing\n",
        "  prob = (count + 1)/(uni_count + len(model_uni))\n",
        "  log_prob = math.log(prob)\n",
        "\n",
        "  model_bigram_prob_smooth[bigram] = log_prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4T69mvYx0Yd",
        "colab_type": "text"
      },
      "source": [
        "Write a text generation function:\n",
        "- takes a bigram as input and generates the next token\n",
        "- iteratively slide the prefix over the generated text so that the new prefix includes the most recent token; \n",
        "- generates the next token\n",
        "- to generate each next token, sample the list of words associated with the prefix using the probability distribution of the prefix.\n",
        "- stop the text generation when a certain number of words have been generated or the latest token is a </s>.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZrHA0ehyvBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# only unigram input\n",
        "\n",
        "def predict_word(model, start = '<s>', tau = 0.5):\n",
        "   \n",
        "   # find all bigrams that begin with 'start'\n",
        "   # return max prob word\n",
        "\n",
        "   probs = list(model.values())\n",
        "   bigrams = list(model.keys())\n",
        "   next_bigrams = [item for item in bigrams if item[0] == start]\n",
        "   \n",
        "   #temperarture sampling\n",
        "   prob_sum = [math.pow(math.exp(item), 1/tau) for item in probs]\n",
        "   prob_sum = sum(prob_sum)\n",
        "\n",
        "   max_prob = 0\n",
        "   next_word = ''\n",
        "  \n",
        "   for bigram in next_bigrams:\n",
        "       prob = math.exp(model[bigram])\n",
        "       \n",
        "       # temperature sampling, tau\n",
        "       prob = math.pow(prob, 1/tau) / prob_sum\n",
        "       \n",
        "       if prob >= max_prob:\n",
        "           max_prob = prob\n",
        "           next_word = bigram[1]\n",
        "\n",
        "   return next_word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGQ-1-BmG2qe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# only unigram input\n",
        "\n",
        "def generate_text(model, max_length = 7, start = '<s>', end = '</s>', tau = 0.5):\n",
        "  \n",
        "  word = ''\n",
        "  num_words = 0\n",
        "  generated_text = start\n",
        "  \n",
        "  while word != end:\n",
        "    word = predict_word(model = model, start = start, tau = tau)\n",
        "    generated_text = generated_text + ' ' + word\n",
        "    #print(word)\n",
        "    start = word\n",
        "    num_words +=1\n",
        "    if num_words > max_length:\n",
        "      break\n",
        "\n",
        "  return generated_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCQRXOOj3_xy",
        "colab_type": "code",
        "outputId": "a95164f1-aae0-4ee2-95ad-2c5dd2c91b5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "s = generate_text(model_bigram_prob, tau = 0.5)\n",
        "s"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<s> i ' s a good idea of the\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlpQaBKtICHD",
        "colab_type": "code",
        "outputId": "a2e8adbf-81a7-4f71-a5cc-d5c0a4ea8f62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "s = generate_text(model_bigram_prob, start = 'probability', tau = 0.5)\n",
        "s"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'probability of the same as a good idea of'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5qWbxdDyE28",
        "colab_type": "text"
      },
      "source": [
        "Write a function that can estimate the probability of a sentence and use it to select the most probable sentence out of several candidate sentences\n",
        "\n",
        "Split the sentence into trigrams and use the chain rule to calculate the probability of the sentence as a product of the bigrams - tokens probabilities\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGIm8QNL3TPu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bigram_sent_split(sentence): \n",
        "  sent = prepare_sent(sentence)\n",
        "  sent_bigrams = list(bigrams(sent[0]))\n",
        "\n",
        "  return sent_bigrams\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKAfbpvi-zgW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "def sent_prob(sentence, model):\n",
        "  sent = bigram_sent_split(sentence)\n",
        "  #sent.reverse()\n",
        "\n",
        "  sent_prob = 0\n",
        "  for bigram in sent:\n",
        "    prob = model[bigram]\n",
        "\n",
        "    #print(prob)\n",
        "    sent_prob  = sent_prob + prob\n",
        "\n",
        "  return math.exp(sent_prob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fov4y0c_7mXv",
        "colab_type": "code",
        "outputId": "34dc69b8-3b45-4544-8205-4493a534580e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "sprob = sent_prob(\"thanks for help\", model_bigram_prob) # TO DO: unknown bigrams!\n",
        "\n",
        "print(sprob)\n",
        "print('{:.20f}'.format(sprob))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8.344074841251085e-08\n",
            "0.00000008344074841251\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-PkSFyqmZI9",
        "colab_type": "code",
        "outputId": "727aaa08-e667-4630-b669-290dc22d74ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "sprob = sent_prob(\"thanks for help\", model_bigram_prob_smooth) # TO DO: unknown bigrams!\n",
        "\n",
        "print(sprob)\n",
        "print('{:.20f}'.format(sprob))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.622204560930194e-09\n",
            "0.00000000362220456093\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLAmnsySyI4C",
        "colab_type": "text"
      },
      "source": [
        "Implement the perplexity scoring function for a given sentence and for the training corpus.\n",
        "Implement Additive Laplace smoothing to give a non zero probability to missing prefix - token combinations when calculating perplexity.\n",
        "\n",
        "Calculate the perplexity of the language model on the test set composed of titles.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86HeOllCMiMA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sent_perplexity(sentence, model):\n",
        "\n",
        "  sent = prepare_sent(sentence)\n",
        "  #print(sent)\n",
        "  s_length = len(sent[0])\n",
        "  \n",
        "  #print(\"length:\", s_length)\n",
        "  \n",
        "  sent_bigrams = list(bigrams(sent[0]))\n",
        "  #print(sent_bigrams)\n",
        "  \n",
        "  sent_prob = 0\n",
        "  for bigram in sent_bigrams:\n",
        "    log_prob = model[bigram]\n",
        "    #print(prob)\n",
        "    sent_prob  = sent_prob + log_prob\n",
        "\n",
        "  \n",
        "  sent_prob = math.exp(sent_prob)\n",
        "  #print(\"sent_prob:\", sent_prob)\n",
        "\n",
        "  perplexity = math.pow(sent_prob, -1/(s_length-1))\n",
        "\n",
        "  return perplexity\n",
        "  \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DpvPeUgN2Wc",
        "colab_type": "code",
        "outputId": "67cfa3e6-7fd4-4879-e6d4-a8a54de03192",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# selected sentence\n",
        "s"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'probability of the same as a good idea of'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oya57dH-Nbm5",
        "colab_type": "code",
        "outputId": "52f4dc2a-d4a4-4189-fd04-c23a885d2035",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "s_perplex = sent_perplexity(s, model_bigram_prob)\n",
        "s_perplex"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34.69041524825544"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqX64Shvozi_",
        "colab_type": "code",
        "outputId": "15eeb40c-7fc5-48f1-edb0-67bd361d26e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# with laplace smoothing\n",
        "\n",
        "s_perplex = sent_perplexity(s, model_bigram_prob_smooth)\n",
        "s_perplex"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "70.17752957486314"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOjBNrVkOJVO",
        "colab_type": "text"
      },
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqHrk1dvyLEe",
        "colab_type": "text"
      },
      "source": [
        "Try to improve the perplexity score of your model by\n",
        "modifying the preprocessing phase of the corpus\n",
        "increasing or decreasing number of tokens in the model (bi grams, 4-grams, …)\n",
        "varying the delta parameter in the Additive Laplace smoothing step."
      ]
    }
  ]
}